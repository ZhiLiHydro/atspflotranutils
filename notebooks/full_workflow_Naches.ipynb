{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "68c4f8ea",
   "metadata": {
    "papermill": {
     "duration": 0.136685,
     "end_time": "2022-03-22T21:32:19.613337",
     "exception": false,
     "start_time": "2022-03-22T21:32:19.476652",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Complete Workflow\n",
    "\n",
    "This workflow provides a complete working example to develop an unstructured mesh for an integrated hydrologic model based on HUCs.  It is the default workflow for integrated hydrology simulations for Exasheds Simulation Campaign 2.\n",
    "\n",
    "It uses the following datasets:\n",
    "\n",
    "* `NHD Plus` for the watershed boundary and hydrography.\n",
    "* `NED` for elevation\n",
    "* `NLCD` for land cover/transpiration/rooting depths\n",
    "* `GLYHMPS` geology data for structural formations\n",
    "* `SoilGrids 2017` for depth to bedrock and soil texture information\n",
    "* `SSURGO` for soil data, where available, in the top 2m.\n",
    "\n",
    "Given some basic inputs (in the next cell) including a NAME, this workflow creates the following files (noting that some suffixes may be appended to the user-provided NAME in homogeneous cases):\n",
    "\n",
    "* Mesh file: `{NAME}.exo`, includes all labeled sets\n",
    "* Forcing: DayMet data -- daily raster of precip, RH, incoming radiation, etc.\n",
    "  - `{NAME}_DayMet_1980_2023.h5`, the DayMet data on this watershed\n",
    "  - `{NAME}_DayMet_typical_1980_2023.h5`, a \"typical year\" of DayMet, smoothed for spinup purposes, then looped 40 years\n",
    "* Forcing: LAI data -- every 4 days, time series by land cover type of LAI.  Note, the raw inputs to this are not done by NAME, but by an (optional, defaults to NAME) MODIS_NAME variable.  Since WW does not currently download MODIS, one might want to use a file of a different name to provide MODIS data.  The times of this MODIS data are hard-coded too -- this is all a bit wonky and will remain so until we get around to adding a file manager for MODIS data.\n",
    "  - `{NAME}_MODIS_LAI_smoothed_2002_2023.h5`, the LAI, interpolated and smoothed from the raw MODIS data\n",
    "  - `{NAME}_MODIS_LAI_typical_1980_2023.h5`, a \"typical year\" of LAI, smoothed for spinup purposes then looped 40 years\n",
    "* Input files: ATS xml files\n",
    "  - `spinup-steadystate-{NAME}.xml` the steady-state solution based on uniform application of mean rainfall rate\n",
    "  - `spinup-cyclic_steadystate-{NAME}.xml` the cyclic steady state based on typical years\n",
    "  - `transient-{NAME}.xml` the forward model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13880ecd-7fba-4d49-ac3a-dfd1d563c47a",
   "metadata": {
    "papermill": {
     "duration": 0.170701,
     "end_time": "2022-03-22T21:32:19.912801",
     "exception": false,
     "start_time": "2022-03-22T21:32:19.742100",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# these can be turned on for development work\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c9ced4a-26fe-4021-bc05-edce44613349",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "name = 'Naches' # name the domain, used in filenames, etc\n",
    "hucs = ['17030002'] # a list of HUCs to run\n",
    "def get_huc12(hucs):\n",
    "    huc12_list = []\n",
    "    for huc in hucs:\n",
    "        if len(huc) == 12:\n",
    "            huc12_list.append(huc)\n",
    "        elif len(huc) == 10:\n",
    "            for i in range(1,20):\n",
    "                huc12_list.append(huc+str(i).zfill(2))\n",
    "        elif len(huc) == 8:\n",
    "            for i in range(1,20):\n",
    "                for j in range(1,20):\n",
    "                    huc12_list.append(huc+str(i).zfill(2)+str(j).zfill(2))\n",
    "        else:\n",
    "            print('need huc8, huc10 or huc12')\n",
    "    return huc12_list\n",
    "hucs = get_huc12(hucs)\n",
    "print(hucs[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "390417a1-1fd3-4932-8d05-4777c9655829",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fire = True\n",
    "if fire:\n",
    "    bs_ptcloud_file_path = f'../data-processed/{name}/fire/xyz.xyz'\n",
    "    perimeter_file_path = f'../data-processed/{name}/fire/perimeter.i2s'\n",
    "    postfire_perm_file_path = f'../data-processed/{name}/fire/postfire_perm.txt'\n",
    "    postfire_perm_4d_file_path = f'../data-processed/{name}/fire/post.xyz'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd98b139",
   "metadata": {
    "papermill": {
     "duration": 0.153266,
     "end_time": "2022-03-22T21:32:20.196694",
     "exception": false,
     "start_time": "2022-03-22T21:32:20.043428",
     "status": "completed"
    },
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# Parameters cell -- this provides all parameters that can be changed via pipelining to generate a new watershed.\n",
    "huc_level = 12 # if provided, an int setting the level at which to include HUC boundaries\n",
    "\n",
    "# geometric parameters\n",
    "simplify_hucs = 100 # length scale to target average edge\n",
    "simplify_rivers = 100\n",
    "stream_outlet_width = 300 # half-width to track a labeled set on which to get discharge\n",
    "ignore_small_rivers = 2 # ignore rivers which have this or fewer reaches.  likely they are irrigation ditches\n",
    "                        # or other small features which make things complicated but likely don't add much value\n",
    "prune_by_area_fraction = 0.01 # ignore reaches whose accumulated catchment area is less than this fraction of the\n",
    "                              # full domain's area\n",
    "prune_by_area_fraction_waterbodies = None\n",
    "num_smoothing_sweeps = 5 # number of times to smooth the DEM prior to elevating\n",
    "\n",
    "# simulation control\n",
    "start_year = 1980  # year to start and end simulation simulation -- note these start and end Oct 1 of the year\n",
    "end_year = 2023\n",
    "min_porosity = 0.05 # minimum porosity considered too small\n",
    "max_permeability = 1.e-10 # max value allowed for permeability\n",
    "max_vg_alpha = 1.e-3 # max value of van Genuchten's alpha -- our correlation is not valid for some soils\n",
    "\n",
    "# triangle refinement control\n",
    "include_rivers = True\n",
    "# refine_d0 = 100\n",
    "# refine_d1 = 500\n",
    "# refine_A0 = 8000\n",
    "# refine_A1 = 50000\n",
    "meshsize = 250\n",
    "factor = 4\n",
    "refine_d0 = meshsize*3\n",
    "refine_A0 = meshsize**2/2\n",
    "refine_d1 = meshsize*30\n",
    "refine_A1 = (np.round(meshsize*factor))**2/2\n",
    "\n",
    "# soil structure\n",
    "use_geologic_layer = True\n",
    "\n",
    "# logistics\n",
    "generate_plots = True # plots take time to make and aren't always needed\n",
    "generate_daymet = True # potentially don't do Met data forcing\n",
    "generate_modis = False\n",
    "\n",
    "include_heterogeneous = True\n",
    "include_homogeneous = False # if true, also write files for homogeneous runs\n",
    "include_homogeneous_wrm = False # if true, also write files for homogeneous WRMs\n",
    "include_homogeneous_wrm_porosity = False # if true, also write files for homogeneous porosity and WRMs\n",
    "include_homogeneous_wrm_permeability = False # if true, also write files for homogeneous perm and WRMs\n",
    "\n",
    "log_to_file = False  # if true, write to file instead of in the notebook output\n",
    "figsize = (6,6)\n",
    "figsize_3d = (8,6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c39048e",
   "metadata": {
    "papermill": {
     "duration": 0.154735,
     "end_time": "2022-03-22T21:32:20.765091",
     "exception": false,
     "start_time": "2022-03-22T21:32:20.610356",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# parameter checking\n",
    "# assert(simplify_hucs > 0 and simplify_hucs < 300)\n",
    "assert(ignore_small_rivers == None or (ignore_small_rivers >= 0 and ignore_small_rivers <= 100))\n",
    "assert(prune_by_area_fraction == None or (prune_by_area_fraction >= 0 and prune_by_area_fraction < 1))\n",
    "# assert(start_year >= 1980 and start_year < 2023)\n",
    "\n",
    "if type(hucs) is str:\n",
    "    assert(hucs[0] == '[')\n",
    "    assert(hucs[-1] == ']')\n",
    "    hucs = hucs[1:-1]\n",
    "    hucs = hucs.split(',')\n",
    "    hucs = [h.strip() for h in hucs]\n",
    "    if hucs[-1] == '':\n",
    "        hucs = hucs[:-1]\n",
    "\n",
    "if huc_level is None:\n",
    "    huc_level = len(hucs[0])\n",
    "else:\n",
    "    assert(huc_level >= len(hucs[0]))\n",
    "huc_key = f'HUC{huc_level}'\n",
    "\n",
    "if prune_by_area_fraction_waterbodies is None:\n",
    "    prune_by_area_fraction_waterbodies = prune_by_area_fraction * 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9695d127",
   "metadata": {
    "papermill": {
     "duration": 0.151242,
     "end_time": "2022-03-22T21:32:21.045921",
     "exception": false,
     "start_time": "2022-03-22T21:32:20.894679",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# a dictionary of outputs -- will include all filenames generated\n",
    "outputs = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6e74e65",
   "metadata": {
    "papermill": {
     "duration": 2.40662,
     "end_time": "2022-03-22T21:32:23.583124",
     "exception": false,
     "start_time": "2022-03-22T21:32:21.176504",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# conda package imports\n",
    "import os,sys\n",
    "import numpy as np\n",
    "import shapely\n",
    "from shapely.geometry import mapping\n",
    "import fiona\n",
    "import logging\n",
    "import scipy.ndimage\n",
    "from matplotlib import pyplot as plt\n",
    "plt.rcParams['figure.dpi'] = 150\n",
    "import h5py\n",
    "import pandas\n",
    "pandas.options.display.max_columns = None\n",
    "pandas.options.display.max_rows = 20\n",
    "\n",
    "# Watershed Workflow\n",
    "import watershed_workflow\n",
    "import watershed_workflow.source_list\n",
    "import watershed_workflow.ui\n",
    "import watershed_workflow.colors\n",
    "import watershed_workflow.condition\n",
    "import watershed_workflow.mesh\n",
    "import watershed_workflow.split_hucs\n",
    "import watershed_workflow.soil_properties\n",
    "import watershed_workflow.daymet\n",
    "import watershed_workflow.utils\n",
    "\n",
    "import watershed_workflow.regions\n",
    "\n",
    "if log_to_file:\n",
    "    outputs['logfile'] = f'{name}.log'\n",
    "    # is this right?  the file handle will become stale... test once this is pipelined.\n",
    "    with open(outputs['logfile'], 'w') as fid:\n",
    "        watershed_workflow.ui.setup_logging(1,fid)\n",
    "else:\n",
    "    watershed_workflow.ui.setup_logging(1,None)\n",
    "\n",
    "# ats_input_spec library, to be moved to amanzi_xml\n",
    "import ats_input_spec\n",
    "import ats_input_spec.public\n",
    "import ats_input_spec.io\n",
    "\n",
    "# amanzi_xml, included in AMANZI_SRC_DIR/tools/amanzi_xml\n",
    "import amanzi_xml.utils.io as aio\n",
    "import amanzi_xml.utils.search as asearch\n",
    "import amanzi_xml.utils.errors as aerrors\n",
    "\n",
    "# from $ATS_SRC_DIR/tools/utils\n",
    "#import smooth_met_box\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93821426",
   "metadata": {
    "papermill": {
     "duration": 0.173444,
     "end_time": "2022-03-22T21:32:23.892314",
     "exception": false,
     "start_time": "2022-03-22T21:32:23.718870",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Note that, by default, we tend to work in the DayMet CRS because this allows us to avoid\n",
    "# reprojecting meteorological forcing datasets.\n",
    "crs = watershed_workflow.crs.daymet_crs()\n",
    "crs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6569dcd",
   "metadata": {
    "papermill": {
     "duration": 0.138411,
     "end_time": "2022-03-22T21:32:24.168402",
     "exception": false,
     "start_time": "2022-03-22T21:32:24.029991",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Sources and setup\n",
    "\n",
    "Next we set up the source watershed and coordinate system and all data sources for our mesh.  We will use the CRS that is included in the shapefile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52ffc4f3",
   "metadata": {
    "papermill": {
     "duration": 0.171121,
     "end_time": "2022-03-22T21:32:24.473705",
     "exception": false,
     "start_time": "2022-03-22T21:32:24.302584",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "logging.info(\"\")\n",
    "logging.info(f\"Meshing shape: {hucs}\")\n",
    "logging.info(\"=\"*30)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "448bb7ea",
   "metadata": {
    "papermill": {
     "duration": 0.184957,
     "end_time": "2022-03-22T21:32:24.790846",
     "exception": false,
     "start_time": "2022-03-22T21:32:24.605889",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "A wide range of data sources are available; here we use the defaults except for using NHD Plus for watershed boundaries and hydrography (the default is NHD, which is lower resolution and therefore smaller download sizes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17feb6e0",
   "metadata": {
    "papermill": {
     "duration": 0.178218,
     "end_time": "2022-03-22T21:32:25.113230",
     "exception": false,
     "start_time": "2022-03-22T21:32:24.935012",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# set up a dictionary of source objects\n",
    "sources = watershed_workflow.source_list.get_default_sources()\n",
    "sources['hydrography'] = watershed_workflow.source_list.hydrography_sources['NHD Plus']\n",
    "sources['HUC'] = watershed_workflow.source_list.huc_sources['NHD Plus']\n",
    "# sources['DEM'] = watershed_workflow.source_list.dem_sources['NED 1/3 arc-second']\n",
    "sources['geologic structure'] = watershed_workflow.source_list.FileManagerGLHYMPS('/global/cfs/cdirs/m1800/zhi/ww/scripts/data/soil_structure/GLHYMPS/GLHYMPS.shp')\n",
    "sources['depth to bedrock'] = watershed_workflow.source_list.FileManagerRaster('/global/cfs/cdirs/m1800/zhi/ww/scripts/data/soil_structure/SoilGrids2017/BDTICM_M_250m_ll.tif')\n",
    "watershed_workflow.source_list.log_sources(sources)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9dc8052",
   "metadata": {
    "papermill": {
     "duration": 1.114087,
     "end_time": "2022-03-22T21:32:26.365716",
     "exception": false,
     "start_time": "2022-03-22T21:32:25.251629",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# load the huc\n",
    "my_hucs = []\n",
    "for huc in hucs:\n",
    "    _, ws = watershed_workflow.get_hucs(sources['HUC'], huc, huc_level, crs)\n",
    "    my_hucs.extend(ws)\n",
    "\n",
    "watershed = watershed_workflow.split_hucs.SplitHUCs(my_hucs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b16639d5-c81c-4897-99d1-cb086eabe2a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "### save watershed boundary shapefile\n",
    "try:\n",
    "    os.mkdir(f'../data-processed/{name}')\n",
    "except FileExistsError:\n",
    "    pass\n",
    "try:\n",
    "    os.mkdir(f'../images/{name}')\n",
    "except FileExistsError:\n",
    "    pass\n",
    "outputs['watershed_shapefile_filename'] = f'../data-processed/{name}/{name}_bounds.shp'\n",
    "# Define a polygon feature geometry with one attribute\n",
    "schema = {\n",
    "    'geometry': 'Polygon',\n",
    "    'properties': {'id': 'int'},\n",
    "}\n",
    "\n",
    "# Write a new Shapefile\n",
    "with fiona.open(outputs['watershed_shapefile_filename'], 'w', 'ESRI Shapefile', schema, crs=watershed_workflow.crs.to_fiona(crs)) as c:\n",
    "    ## If there are multiple geometries, put the \"for\" loop here\n",
    "    c.write({\n",
    "        'geometry': mapping(watershed.exterior()),\n",
    "        'properties': {'id': 123},\n",
    "    })"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cf9cb0b",
   "metadata": {
    "papermill": {
     "duration": 0.175642,
     "end_time": "2022-03-22T21:32:26.710144",
     "exception": false,
     "start_time": "2022-03-22T21:32:26.534502",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Generate the surface mesh\n",
    "\n",
    "First we'll generate the flattened, 2D triangulation, which builds on hydrography data.  Then we download a digital elevation map from the National Elevation Dataset, and extrude that 2D triangulation to a 3D surface mesh based on interpolation between pixels of the DEM."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c64ab13",
   "metadata": {
    "papermill": {
     "duration": 0.161358,
     "end_time": "2022-03-22T21:32:27.040288",
     "exception": false,
     "start_time": "2022-03-22T21:32:26.878930",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Get river network\n",
    "\n",
    "This will download the river network from the NHD Plus database, and simplify the network, constructing a tree-like data structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a029c68",
   "metadata": {
    "papermill": {
     "duration": 13.879677,
     "end_time": "2022-03-22T21:32:41.083632",
     "exception": false,
     "start_time": "2022-03-22T21:32:27.203955",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if include_rivers:  \n",
    "    # download/collect the river network within that shape's bounds\n",
    "    _, reaches = watershed_workflow.get_reaches(sources['hydrography'], huc, \n",
    "                                                watershed.exterior(), crs, crs,\n",
    "                                                in_network=True, properties=True)\n",
    "    \n",
    "    rivers = watershed_workflow.construct_rivers(reaches, method='hydroseq',\n",
    "                                                 ignore_small_rivers=ignore_small_rivers,\n",
    "                                                 prune_by_area=prune_by_area_fraction * watershed.exterior().area * 1.e-6,\n",
    "                                                 remove_diversions=True,\n",
    "                                                 remove_braided_divergences=True)\n",
    "else:\n",
    "    reaches = []\n",
    "    rivers = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e7d2200-a2b6-4f48-8509-020cf9aad290",
   "metadata": {
    "papermill": {
     "duration": 2.833493,
     "end_time": "2022-03-22T21:32:44.891767",
     "exception": false,
     "start_time": "2022-03-22T21:32:42.058274",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# what about the reservoir!\n",
    "_, waterbodies = watershed_workflow.get_waterbodies(sources['hydrography'], huc, watershed.exterior(), crs, crs, \n",
    "                                                    prune_by_area=prune_by_area_fraction_waterbodies*watershed.exterior().area)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0c321e6-7361-4a40-b996-f87fcf0f7a45",
   "metadata": {
    "papermill": {
     "duration": 1.276371,
     "end_time": "2022-03-22T21:32:46.919877",
     "exception": false,
     "start_time": "2022-03-22T21:32:45.643506",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if generate_plots:\n",
    "    fig, ax = watershed_workflow.plot.get_ax(crs)\n",
    "    watershed_workflow.plot.hucs(watershed, crs, 'k', ax)\n",
    "    watershed_workflow.plot.rivers(rivers, crs, 'b', ax)\n",
    "    watershed_workflow.plot.shplys(waterbodies, crs, 'g', ax, facecolor='c')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "464e8f07-ef0e-45d3-8656-89b29f61bb13",
   "metadata": {
    "papermill": {
     "duration": 1.54641,
     "end_time": "2022-03-22T21:32:49.247319",
     "exception": false,
     "start_time": "2022-03-22T21:32:47.700909",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # simplify the geometry for a \"nicer\" discrete object\n",
    "# watershed_workflow.simplify(watershed, rivers, waterbodies, simplify_hucs=simplify_hucs, simplify_rivers=simplify_rivers,\n",
    "#                             snap=False, cut_intersections=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f3952a0-a3b8-4b68-b9c6-2ee06d12787d",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "tol = 1e-3\n",
    "import copy\n",
    "# keeping the originals\n",
    "rivers_orig=[river.deepcopy() for river in rivers]\n",
    "watershed_orig=copy.deepcopy(watershed) \n",
    "\n",
    "# simplifying \n",
    "rivers = watershed_workflow.simplify(watershed, rivers, \n",
    "                                     simplify_hucs=simplify_hucs, \n",
    "                                     simplify_rivers=simplify_rivers, \n",
    "                                     prune_tol=tol, merge_tol=tol, snap_tol=tol,\n",
    "                                     cut_intersections=False)\n",
    "\n",
    "# for plotting purpose only\n",
    "rivers_simplified=[river.deepcopy() for river in rivers] \n",
    "watershed_simplified=copy.deepcopy(watershed) \n",
    "\n",
    "print('number of reaches in original', len(rivers_orig[0]), 'number of reaches in simplified', len(rivers[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7550ba9",
   "metadata": {
    "papermill": {
     "duration": 1.696508,
     "end_time": "2022-03-22T21:32:51.937434",
     "exception": false,
     "start_time": "2022-03-22T21:32:50.240926",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Generate meshes using river network and watershed shape\n",
    "\n",
    "Triangulation refinement: refine triangles if their area (in m^2) is greater than A(d), where d is the \n",
    "distance from the triangle centroid to the nearest stream.  A(d) is a piecewise linear function -- A = A0 if d <= d0, A = A1 if d >= d1, and linearly interpolates between the two endpoints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d72ecff",
   "metadata": {
    "papermill": {
     "duration": 55.034693,
     "end_time": "2022-03-22T21:33:48.294584",
     "exception": false,
     "start_time": "2022-03-22T21:32:53.259891",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "d0 = refine_d0; d1 = refine_d1\n",
    "A0 = refine_A0; A1 = refine_A1 # [100, 310]m\n",
    "\n",
    "# Refine triangles if they get too acute\n",
    "min_angle = 32 # degrees\n",
    "\n",
    "# make 2D mesh\n",
    "diagnostics = True\n",
    "if generate_plots and diagnostics:\n",
    "    # mesh_points2, mesh_tris, areas, distances = watershed_workflow.triangulate(watershed, rivers, \n",
    "    #                                                refine_distance=[d0,A0,d1,A1],\n",
    "    #                                                refine_min_angle=min_angle,\n",
    "    #                                                diagnostics=True)\n",
    "    mesh_points2, mesh_tris, areas, distances, river_idx = watershed_workflow.triangulate(watershed, rivers,                                                 \n",
    "                                                   refine_distance=[d0,A0,d1,A1],\n",
    "                                                   refine_min_angle=min_angle,\n",
    "                                                   # enforce_delaunay=True,\n",
    "                                                   diagnostics=True,\n",
    "                                                   river_region_dist=meshsize)\n",
    "else:\n",
    "    mesh_points2, mesh_tris = watershed_workflow.triangulate(watershed, rivers, \n",
    "                                                   refine_distance=[d0,A0,d1,A1],\n",
    "                                                   refine_min_angle=min_angle,\n",
    "                                                   diagnostics=False)\n",
    "print('total area=', np.sum(areas), 'm^2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4ccbff7-c964-4601-8cfa-bb3bb86d33cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "river_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "456614e4-8fae-4a60-b4ab-9b98e19ac5e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "str(len(river_idx[river_idx])/len(river_idx)*100)+'%'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d417dcc",
   "metadata": {
    "papermill": {
     "duration": 0.910373,
     "end_time": "2022-03-22T21:33:50.151963",
     "exception": false,
     "start_time": "2022-03-22T21:33:49.241590",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Map mesh to DEM\n",
    "\n",
    "Download a DEM from USGS NED and elevate the triangle nodes to the DEM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb69b25a",
   "metadata": {
    "papermill": {
     "duration": 1.313605,
     "end_time": "2022-03-22T21:33:52.380596",
     "exception": false,
     "start_time": "2022-03-22T21:33:51.066991",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# download the needed rasters\n",
    "dem_profile, dem = watershed_workflow.get_raster_on_shape(sources['DEM'], watershed.exterior(), crs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1307ce08",
   "metadata": {
    "papermill": {
     "duration": 1.779412,
     "end_time": "2022-03-22T21:33:55.097939",
     "exception": false,
     "start_time": "2022-03-22T21:33:53.318527",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# noting that the DEM is a 30m raster, and we want to run at a coarser resolution of ~100-300m, \n",
    "# the DEM will look quite rough.  Smooth a small amount.  Note better algorithms could be used \n",
    "# here, but for now we just use Gaussian smoothing.\n",
    "if num_smoothing_sweeps > 0:\n",
    "    dem_sm = scipy.ndimage.gaussian_filter(dem, num_smoothing_sweeps, mode='nearest')\n",
    "else:\n",
    "    dem_sm = dem[:]\n",
    "\n",
    "if generate_plots:\n",
    "    fig, axs = plt.subplots(1,2, figsize=(10,5))\n",
    "    axs[0].imshow(dem)\n",
    "    axs[0].set_title('unsmoothed')\n",
    "    axs[1].imshow(dem_sm)\n",
    "    txt = axs[1].set_title('smoothed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66dd4a7e",
   "metadata": {
    "papermill": {
     "duration": 1.359689,
     "end_time": "2022-03-22T21:33:57.401647",
     "exception": false,
     "start_time": "2022-03-22T21:33:56.041958",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# elevate the x,y points onto the DEM to get a z coordinate\n",
    "mesh_points3 = watershed_workflow.elevate(mesh_points2, crs, dem_sm, dem_profile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb7e0bfa",
   "metadata": {
    "papermill": {
     "duration": 1.948678,
     "end_time": "2022-03-22T21:34:00.259786",
     "exception": false,
     "start_time": "2022-03-22T21:33:58.311108",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# construct the 2D mesh\n",
    "m2 = watershed_workflow.mesh.Mesh2D(mesh_points3.copy(), list(mesh_tris), crs=crs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a4b1275",
   "metadata": {
    "papermill": {
     "duration": 35.370726,
     "end_time": "2022-03-22T21:34:36.563138",
     "exception": false,
     "start_time": "2022-03-22T21:34:01.192412",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# hydrologically condition the mesh, removing pits.\n",
    "#\n",
    "# replace conditioned mesh where there are water bodies!\n",
    "all_bodies = shapely.geometry.MultiPolygon(waterbodies)\n",
    "\n",
    "waterbody_mask = np.zeros((len(m2.conn),), 'i')\n",
    "for i,p in enumerate(m2.centroids):\n",
    "    if all_bodies.contains(shapely.geometry.Point(p)):\n",
    "        waterbody_mask[i] = 1\n",
    "        \n",
    "print(waterbody_mask.shape)\n",
    "print(waterbody_mask.sum())\n",
    "# NOTE: this fills reservoirs as well!  Might have to think about how to allow some pits!\n",
    "# watershed_workflow.condition.fill_pits_dual(m2, is_waterbody=waterbody_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7aab82f-a306-498c-b202-2f7e3a8099a3",
   "metadata": {
    "papermill": {
     "duration": 2.325046,
     "end_time": "2022-03-22T21:34:39.899367",
     "exception": false,
     "start_time": "2022-03-22T21:34:37.574321",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# image the diff of the elevation\n",
    "if generate_plots:\n",
    "    diff = m2.coords[:,2] - mesh_points3[:,2]\n",
    "    \n",
    "    fig, ax = watershed_workflow.plot.get_ax(crs, figsize=(12,10))\n",
    "\n",
    "    mp = watershed_workflow.plot.triangulation(m2.coords, m2.conn, crs, ax=ax, \n",
    "                                 color=diff, edgecolor='white', linewidth=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bf24aa1-83f6-4397-82ea-24034831eee1",
   "metadata": {
    "papermill": {
     "duration": 1.509415,
     "end_time": "2022-03-22T21:34:42.380281",
     "exception": false,
     "start_time": "2022-03-22T21:34:40.870866",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# identify outlets by the elevation map\n",
    "#watershed_workflow.split_hucs.find_outlets_by_elevation(watershed, crs, dem_sm, dem_profile)\n",
    "\n",
    "print(len(rivers))\n",
    "rivers = sorted(rivers, key=len)\n",
    "watershed_workflow.split_hucs.find_outlets_by_hydroseq(watershed, rivers[-1])\n",
    "\n",
    "if generate_plots:\n",
    "    fig, ax = watershed_workflow.plot.get_ax(crs, figsize=(12,10))\n",
    "    colors = watershed_workflow.colors.enumerated_colors(len(watershed), palette=4)\n",
    "    watershed_workflow.plot.hucs(watershed, crs, ax=ax, color=colors, linewidth=1, facecolor='color', alpha=0.4)\n",
    "    watershed_workflow.plot.rivers(rivers, crs, ax=ax, colors='b', linewidth=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b75d05e",
   "metadata": {
    "papermill": {
     "duration": 3.083096,
     "end_time": "2022-03-22T21:34:46.503856",
     "exception": false,
     "start_time": "2022-03-22T21:34:43.420760",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "if generate_plots:\n",
    "    # plot the resulting surface mesh\n",
    "    fig, ax = watershed_workflow.plot.get_ax(crs, window=[0.05,0.1,0.9,0.8], figsize=(12,10))\n",
    "    cbax = fig.add_axes([0.05,0.05,0.9,0.05])\n",
    "    \n",
    "    mp = watershed_workflow.plot.triangulation(m2.coords, m2.conn, crs, ax=ax, \n",
    "                                 color='elevation', edgecolor='white', linewidth=0.2)\n",
    "    cbar = fig.colorbar(mp, orientation=\"horizontal\", cax=cbax)\n",
    "    watershed_workflow.plot.hucs(watershed, crs, ax=ax, color='k', linewidth=1)\n",
    "    watershed_workflow.plot.rivers(rivers, crs, ax=ax, color='red', linewidth=1)\n",
    "    ax.set_aspect('equal', 'datalim')\n",
    "    txt = cbar.ax.set_xlabel('elevation [m]', loc='center')\n",
    "    fig.savefig(f'../images/{name}/{name}_dem')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f04080eb",
   "metadata": {
    "papermill": {
     "duration": 11.10124,
     "end_time": "2022-03-22T21:34:58.683420",
     "exception": false,
     "start_time": "2022-03-22T21:34:47.582180",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # add labeled sets for subcatchments and outlets\n",
    "# watershed_workflow.mesh.add_watershed_regions_and_outlets(m2, watershed, stream_outlet_width, \n",
    "#                                                           labels=[p.properties[huc_key] for p in watershed.polygons()])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2689e5e2-b805-4d93-9628-cb5948651da2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add labeled sets for subcatchments and outlets\n",
    "watershed_workflow.regions.add_watershed_regions_and_outlets(m2, watershed, outlet_width=stream_outlet_width, \n",
    "                                                          labels=[p.properties[huc_key] for p in watershed.polygons()], exterior_outlet= True)\n",
    "\n",
    "# add labeled sets for river corridor cells\n",
    "# watershed_workflow.regions.add_river_corridor_regions(m2, rivers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21d7f0af-2a5b-4103-86cf-584059643bd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "for ls in m2.labeled_sets:\n",
    "    print(f'{ls.setid} : {ls.entity} : {len(ls.ent_ids)} : \"{ls.name}\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfcb8aa0",
   "metadata": {
    "papermill": {
     "duration": 0.995698,
     "end_time": "2022-03-22T21:35:00.677226",
     "exception": false,
     "start_time": "2022-03-22T21:34:59.681528",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Surface properties\n",
    "\n",
    "Meshes interact with data to provide forcing, parameters, and more in the actual simulation.  Specifically, we need vegetation type on the surface to provide information about transpiration and subsurface structure to provide information about water retention curves, etc.\n",
    "\n",
    "We'll start by downloading and collecting land cover from the NLCD dataset, and generate sets for each land cover type that cover the surface.  Likely these will be some combination of grass, deciduous forest, coniferous forest, and mixed forest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99d8cee0",
   "metadata": {
    "papermill": {
     "duration": 1.827032,
     "end_time": "2022-03-22T21:35:03.448194",
     "exception": false,
     "start_time": "2022-03-22T21:35:01.621162",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# download the NLCD raster\n",
    "lc_profile, lc_raster = watershed_workflow.get_raster_on_shape(sources['land cover'], \n",
    "                                                     watershed.exterior(), crs)\n",
    "\n",
    "# resample the raster to the triangles\n",
    "lc = watershed_workflow.values_from_raster(m2.centroids, crs, lc_raster, lc_profile)\n",
    "\n",
    "# what land cover types did we get?\n",
    "logging.info('Found land cover dtypes: {}'.format(lc.dtype))\n",
    "logging.info('Found land cover types: {}'.format(set(lc)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2814b446-54aa-42db-81b6-42142c6c34b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "### land cover histogram\n",
    "u, count = np.unique(lc, return_counts=True)\n",
    "count_sort_ind = np.argsort(-count)\n",
    "lc_dict = dict(zip(u[count_sort_ind], count[count_sort_ind]))\n",
    "print(lc_dict)\n",
    "aa = count[count_sort_ind]\n",
    "print(aa[0]/np.sum(aa)*100, aa[1]/np.sum(aa)*100, aa[2]/np.sum(aa)*100, aa[3]/np.sum(aa)*100)\n",
    "plt.hist(lc, bins='auto')\n",
    "plt.ylabel('counts')\n",
    "plt.xlabel('index')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea788ead",
   "metadata": {
    "papermill": {
     "duration": 27.905246,
     "end_time": "2022-03-22T21:35:32.341615",
     "exception": false,
     "start_time": "2022-03-22T21:35:04.436369",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if generate_plots:\n",
    "    # plot the image\n",
    "    # -- get the NLCD colormap which uses official NLCD colors and labels\n",
    "    nlcd_indices, nlcd_cmap, nlcd_norm, nlcd_ticks, nlcd_labels = \\\n",
    "                watershed_workflow.colors.generate_nlcd_colormap(lc)\n",
    "\n",
    "    fig = plt.figure(figsize=figsize)\n",
    "    ax = watershed_workflow.plot.get_ax(crs, fig)\n",
    "    polys = watershed_workflow.plot.mesh(m2, crs, ax=ax, color=lc, cmap=nlcd_cmap, norm=nlcd_norm, edgecolor='none', \n",
    "                                     facecolor='color', linewidth=0.5)\n",
    "\n",
    "    # nlcd_labels.pop()\n",
    "    watershed_workflow.colors.colorbar_index(ncolors=len(nlcd_indices), cmap=nlcd_cmap, labels = nlcd_labels) \n",
    "\n",
    "    ax.set_title(\"NLCD land cover index\")\n",
    "    ext = ax.axis('off')\n",
    "    fig.savefig(f'../images/{name}/{name}_land_cover_raw')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a9b54d2-0851-412c-b940-5367817f132a",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlcd_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "659648eb-c7da-4916-bc24-fb0fe39e5643",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlcd_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41b8d97d-d657-4833-81f6-5f3542ffb39c",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(nlcd_indices), len(nlcd_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4698744-03ee-4f3c-b490-f39f7e303795",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlcd_cmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59ec8c9b-eca2-4db3-8595-2e8b0b417e06",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(nlcd_cmap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaf01956-a5ea-42bb-afc6-726e6c25d6c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlcd_labels_dict = dict(zip(nlcd_indices, nlcd_labels))\n",
    "nlcd_labels_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94164938",
   "metadata": {
    "papermill": {
     "duration": 1.055939,
     "end_time": "2022-03-22T21:35:34.403177",
     "exception": false,
     "start_time": "2022-03-22T21:35:33.347238",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# we don't really need all of these.  Keep Evergreen, Deciduous, Shrub, and merge the rest into \"Other\"\n",
    "nlcd_color_new = 99 * np.ones_like(lc)\n",
    "\n",
    "groupings = {\n",
    "    42 : ['Evergreen Forest',],\n",
    "    # 41 : ['Deciduous Forest', 'Mixed Forest', 'Woody Wetlands'],\n",
    "    52 : ['Dwarf Scrub', 'Shrub/Scrub', 'Grassland/Herbaceous', 'Sedge/Herbaceous', \n",
    "                     'Pasture/Hay', 'Cultivated Crops'],\n",
    "    71 : ['Grassland/Herbaceous'],\n",
    "}\n",
    "\n",
    "for k,v in groupings.items():\n",
    "    for label in v:\n",
    "        index = sources['land cover'].indices[label]\n",
    "        nlcd_color_new[np.where(lc == index)] = k\n",
    "    \n",
    "print(nlcd_color_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "982125d5-a5bd-4cc2-b0fe-1f2fb9a59b19",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "### land cover histogram\n",
    "u, count = np.unique(nlcd_color_new, return_counts=True)\n",
    "count_sort_ind = np.argsort(-count)\n",
    "print(u[count_sort_ind])\n",
    "print(count[count_sort_ind])\n",
    "aa = count[count_sort_ind]\n",
    "print(aa[0]/np.sum(aa)*100, aa[1]/np.sum(aa)*100, aa[2]/np.sum(aa)*100)#, aa[3]/np.sum(aa)*100)\n",
    "plt.hist(nlcd_color_new, bins='auto')\n",
    "plt.ylabel('counts')\n",
    "plt.xlabel('index')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faf19cf1",
   "metadata": {
    "papermill": {
     "duration": 27.807974,
     "end_time": "2022-03-22T21:36:03.231449",
     "exception": false,
     "start_time": "2022-03-22T21:35:35.423475",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# plot the updated image, adding \"other\"\n",
    "nlcd_color_new_other_as_water = np.where(nlcd_color_new == 99, 11, nlcd_color_new)\n",
    "\n",
    "# -- get the NLCD colormap which uses official NLCD colors and labels\n",
    "nlcd_indices, nlcd_cmap, nlcd_norm, nlcd_ticks, nlcd_labels = \\\n",
    "            watershed_workflow.colors.generate_nlcd_colormap(nlcd_color_new_other_as_water)\n",
    "\n",
    "# make (water, 11) into (other, 99)\n",
    "nlcd_labels[0] = 'Other'\n",
    "nlcd_indices[0] = 99\n",
    "\n",
    "if generate_plots:\n",
    "    fig, ax = watershed_workflow.plot.get_ax(crs)\n",
    "\n",
    "\n",
    "    polys = watershed_workflow.plot.mesh(m2, crs, ax=ax, color=nlcd_color_new_other_as_water, \n",
    "                           cmap=nlcd_cmap, norm=nlcd_norm, edgecolor='none', \n",
    "                           facecolor='color', linewidth=0.5)\n",
    "    # nlcd_labels.pop()\n",
    "    watershed_workflow.colors.colorbar_index(ncolors=len(np.unique(nlcd_color_new_other_as_water)), \n",
    "                               cmap=nlcd_cmap, labels = nlcd_labels) \n",
    "\n",
    "    ax.set_title(\"Land Cover Index\")\n",
    "    ext = ax.axis('off')\n",
    "    fig.savefig(f'../images/{name}/{name}_land_cover')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cd7e470",
   "metadata": {
    "papermill": {
     "duration": 1.118581,
     "end_time": "2022-03-22T21:36:05.440884",
     "exception": false,
     "start_time": "2022-03-22T21:36:04.322303",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # add labeled sets to the mesh for NLCD\n",
    "# nlcd_labels_dict = dict(zip(nlcd_indices, nlcd_labels))\n",
    "# watershed_workflow.mesh.add_nlcd_labeled_sets(m2, nlcd_color_new, nlcd_labels_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44ccdbec-8d1e-4a54-bc11-f3e2ba04ff47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add labeled sets to the mesh for NLCD\n",
    "nlcd_labels_dict = dict(zip(nlcd_indices, nlcd_labels))\n",
    "watershed_workflow.regions.add_nlcd_labeled_sets(m2, nlcd_color_new, nlcd_labels_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f897d9f-ea50-4367-b928-d47816bc8f65",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "### land cover histogram\n",
    "print(nlcd_labels_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d05d7a8e",
   "metadata": {
    "papermill": {
     "duration": 1.204107,
     "end_time": "2022-03-22T21:36:07.741741",
     "exception": false,
     "start_time": "2022-03-22T21:36:06.537634",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "for ls in m2.labeled_sets:\n",
    "    print(f'{ls.setid} : {ls.entity} : {len(ls.ent_ids)} : \"{ls.name}\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5899cf16",
   "metadata": {
    "papermill": {
     "duration": 1.056483,
     "end_time": "2022-03-22T21:36:09.857849",
     "exception": false,
     "start_time": "2022-03-22T21:36:08.801366",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Subsurface properties\n",
    "\n",
    "The default model uses GLHYMPS to identify geologic formations, and "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "052397f1",
   "metadata": {
    "papermill": {
     "duration": 27.567258,
     "end_time": "2022-03-22T21:36:38.475653",
     "exception": false,
     "start_time": "2022-03-22T21:36:10.908395",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# download the NRCS soils data as shapes and project it onto the mesh\n",
    "#\n",
    "soil_profile, soil_survey, soil_survey_props = \\\n",
    "        watershed_workflow.get_shapes(sources['soil structure'], list(watershed.polygons()), crs, \n",
    "                                                     crs, properties=True)\n",
    "\n",
    "# -- determine the NRCS mukey for each soil unit; this uniquely identifies soil \n",
    "#    properties\n",
    "soil_ids = list(soil_survey_props['mukey'][:])\n",
    "soil_survey_props.set_index('mukey', inplace=True)\n",
    "\n",
    "# -- color a raster by the polygons (this makes identifying a triangle's value much \n",
    "#    more efficient)\n",
    "soil_color_profile, soil_color_raster = watershed_workflow.color_raster_from_shapes(soil_survey, crs, soil_ids,\n",
    "                                                                                    watershed.exterior().bounds, 10, crs, -1)\n",
    "\n",
    "# -- resample the raster to the triangles\n",
    "soil_color = watershed_workflow.values_from_raster(m2.centroids, crs, \n",
    "                                         soil_color_raster, soil_color_profile)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1dadaac-8b77-4c3e-9c6b-1cd77eca7aad",
   "metadata": {
    "papermill": {
     "duration": 1.160188,
     "end_time": "2022-03-22T21:36:40.672424",
     "exception": false,
     "start_time": "2022-03-22T21:36:39.512236",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "soil_survey_props\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6be0afae-5612-4937-ae4c-e8b2d690deac",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('soil_color: ', soil_color)\n",
    "print('soil_color.shape: ', soil_color.shape)\n",
    "print('len(np.unique(soil_ids)): ', len(np.unique(soil_ids)))\n",
    "print('len(np.unique(soil_color)): ', len(np.unique(soil_color)))\n",
    "print('np.unique(soil_ids): ', np.unique(soil_ids))\n",
    "print('np.unique(soil_color): ', np.unique(soil_color))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9781369f",
   "metadata": {
    "papermill": {
     "duration": 29.450096,
     "end_time": "2022-03-22T21:37:11.160197",
     "exception": false,
     "start_time": "2022-03-22T21:36:41.710101",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if generate_plots:\n",
    "    # plot the soil mukey\n",
    "    indices, cmap, norm, ticks, labels = watershed_workflow.colors.generate_indexed_colormap(soil_color, cmap='tab20c')\n",
    "    fig, ax = watershed_workflow.plot.get_ax(crs)\n",
    "\n",
    "    mp = watershed_workflow.plot.mesh(m2, crs, ax=ax, facecolor='color',\n",
    "                            linewidth=0, color=soil_color, \n",
    "                            cmap=cmap, norm = norm\n",
    "                           )\n",
    "\n",
    "    # watershed_workflow.colors.colorbar_index(ncolors=len(np.unique(soil_color)), cmap=cmap, labels=labels) \n",
    "\n",
    "    ax.set_title('soil type index')\n",
    "    ax.axis('off')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3295eab0",
   "metadata": {
    "papermill": {
     "duration": 2.24315,
     "end_time": "2022-03-22T21:37:14.560167",
     "exception": false,
     "start_time": "2022-03-22T21:37:12.317017",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# what does soil thickness look like?\n",
    "soil_thickness = np.nan * np.ones(soil_color.shape, 'd')\n",
    "for mukey in soil_survey_props.index:\n",
    "    soil_thickness[soil_color == mukey] = soil_survey_props.loc[mukey,'thickness [cm]']\n",
    "\n",
    "soil_thickness = soil_thickness / 100\n",
    "soil_thickness = np.where(np.isnan(soil_thickness), 2.0, soil_thickness)\n",
    "\n",
    "if generate_plots:\n",
    "    fig, ax = watershed_workflow.plot.get_ax(crs)\n",
    "    mp = watershed_workflow.plot.triangulation(mesh_points3, mesh_tris, crs, ax=ax, \n",
    "                                 color=soil_thickness, edgecolor='none', cmap='jet')\n",
    "    ax.set_title('soil thickness [m]')\n",
    "    cb = fig.colorbar(mp, fraction=0.04, pad=0.04)\n",
    "    ax.axis('off')\n",
    "\n",
    "print('Median soil thickness [m] = ', np.median(soil_thickness))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a24c30d4",
   "metadata": {
    "papermill": {
     "duration": 2.439565,
     "end_time": "2022-03-22T21:37:18.030396",
     "exception": false,
     "start_time": "2022-03-22T21:37:15.590831",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if generate_plots:\n",
    "    # plot of porosity from SSURGO\n",
    "    iprop = np.empty(soil_color.shape, 'd')\n",
    "    for mukey in soil_survey_props.index:\n",
    "        iprop[soil_color == mukey] = soil_survey_props.loc[ mukey,'porosity [-]']\n",
    "\n",
    "    fig, ax = watershed_workflow.plot.get_ax(crs)\n",
    "    mp = watershed_workflow.plot.triangulation(mesh_points3, mesh_tris, crs, ax=ax, \n",
    "                                 color=iprop, edgecolor='none', cmap='jet')\n",
    "    ax.set_title('soil porosity [-]')\n",
    "    cb = fig.colorbar(mp, fraction=0.04, pad=0.04, extend=\"both\", shrink=0.6)\n",
    "    ax.axis('off')\n",
    "\n",
    "    print('Median porosity [-] = ', np.nanmedian(iprop))\n",
    "    fig.savefig(f'../images/{name}/{name}_soil_porosity')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "500c2b45",
   "metadata": {
    "papermill": {
     "duration": 2.348316,
     "end_time": "2022-03-22T21:37:21.537009",
     "exception": false,
     "start_time": "2022-03-22T21:37:19.188693",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if generate_plots:\n",
    "    # plot of permeability\n",
    "    iprop = np.empty(soil_color.shape, 'd')\n",
    "    for mukey in soil_survey_props.index:\n",
    "        iprop[soil_color == mukey] = soil_survey_props.loc[ mukey,'permeability [m^2]']\n",
    "\n",
    "    fig, ax = watershed_workflow.plot.get_ax(crs)\n",
    "    mp = watershed_workflow.plot.triangulation(mesh_points3, mesh_tris, crs, ax=ax, \n",
    "                                 color=np.log10(iprop), edgecolor='none', cmap='jet')\n",
    "    ax.set_title('soil permeability [m^2]')\n",
    "    cb = fig.colorbar(mp, fraction=0.04, pad=0.04, extend=\"both\", shrink=0.6)\n",
    "    cb.ax.set_title('log K')\n",
    "    ax.axis('off')\n",
    "\n",
    "    print('Min k [m^2] = ', np.nanmin(iprop))\n",
    "    print('Max k [m^2] = ', np.nanmax(iprop))\n",
    "    print('Mean k [m^2] = ', np.nanmean(iprop))\n",
    "    print('Median k [m^2] = ', np.nanmedian(iprop))\n",
    "    fig.savefig(f'../images/{name}/{name}_soil_permeability')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c03fbfbd-96c0-4472-929e-109340c684a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('# of nan: ', len(iprop[np.isnan(iprop)]))\n",
    "for i,j in enumerate(iprop):\n",
    "    if np.isnan(j):\n",
    "        iprop[i] = np.nanmean(iprop)\n",
    "print('# of nan: ', len(iprop[np.isnan(iprop)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fdcfe7d-b87a-4962-bb74-1ca6bf9cc5bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "if generate_plots:\n",
    "    # plot of permeability\n",
    "\n",
    "    fig, ax = watershed_workflow.plot.get_ax(crs)\n",
    "    mp = watershed_workflow.plot.triangulation(mesh_points3, mesh_tris, crs, ax=ax, \n",
    "                                 color=np.log10(iprop), edgecolor='none', cmap='jet')\n",
    "    ax.set_title('nan removed\\nsoil permeability [m^2]')\n",
    "    cb = fig.colorbar(mp, fraction=0.04, pad=0.04, extend=\"both\", shrink=0.6)\n",
    "    cb.ax.set_title('log K')\n",
    "    ax.axis('off')\n",
    "\n",
    "    print('Min k [m^2] = ', np.nanmin(iprop))\n",
    "    print('Max k [m^2] = ', np.nanmax(iprop))\n",
    "    print('Mean k [m^2] = ', np.nanmean(iprop))\n",
    "    print('Median k [m^2] = ', np.nanmedian(iprop))\n",
    "    fig.savefig(f'../images/{name}/{name}_soil_permeability')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3be7f4a4-7fd6-4cfc-aecb-f23cf633c7bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "if fire:\n",
    "    m2 = watershed_workflow.mesh.Mesh2D(mesh_points3.copy(), list(mesh_tris), crs=crs)\n",
    "    xyz = m2.centroids\n",
    "(xyz, xyz.shape) if fire else fire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ef91fe3-ded2-47a0-b348-33ec7c05b6f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "if fire:\n",
    "    from shapely.geometry import Point, Polygon\n",
    "    d = np.loadtxt(bs_ptcloud_file_path)\n",
    "    x, y, z = d[:,0], d[:,1], d[:,2]\n",
    "(d, d.shape) if fire else fire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2da0897-14ff-4209-9a69-175ad7fd6155",
   "metadata": {},
   "outputs": [],
   "source": [
    "if fire:\n",
    "    print('before clean =', np.unique(z))\n",
    "    for i in [0,5,6,7]:\n",
    "        x, y, z = x[z!=i], y[z!=i], z[z!=i]\n",
    "    print('after clean =', np.unique(z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "109841d1-2824-4c8b-ade8-beac2f93837b",
   "metadata": {},
   "outputs": [],
   "source": [
    "if fire:\n",
    "    burn_severity_nodes = [(x[i], y[i]) for i in range(len(x))]\n",
    "    polygon = Polygon(burn_severity_nodes)\n",
    "# polygon if fire else fire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68169a11-9ce5-4689-88d2-efc1329db988",
   "metadata": {},
   "outputs": [],
   "source": [
    "if fire:\n",
    "    perimeter = np.loadtxt(perimeter_file_path)\n",
    "    px, py = perimeter[:,0], perimeter[:,1]\n",
    "(perimeter, perimeter.shape) if fire else fire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cbc4efc-3657-4158-aea8-f8b6afb91d7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "if fire:\n",
    "    fire_perimeter_nodes = [(px[i], py[i]) for i in range(len(px))]\n",
    "    fireperimeter = Polygon(fire_perimeter_nodes)\n",
    "    \n",
    "fireperimeter if fire else fire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30f95d1c-8e67-458d-972f-06f33f8cd2ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "if fire:\n",
    "    fig, ax1 = plt.subplots(1, 1, figsize=(5,5))\n",
    "    diff = np.ones(np.shape(m2.coords[:,2]))\n",
    "    watershed_workflow.plot.triangulation(m2.coords, m2.conn, crs, ax=ax1, color=diff, edgecolor='none', linewidth=0.2, cmap='Blues', vmin=0, vmax=2)\n",
    "    p1 = ax1.scatter(x[z==1], y[z==1], c='white', s=0.001)\n",
    "    p1 = ax1.scatter(x[z==2], y[z==2], c='wheat', s=0.001)\n",
    "    p1 = ax1.scatter(x[z==3], y[z==3], c='salmon', s=0.001)\n",
    "    p1 = ax1.scatter(x[z==4], y[z==4], c='firebrick', s=0.001)\n",
    "    ax1.plot(px, py, 'k')\n",
    "    # ax1.set_title('burn severity - point cloud')\n",
    "    ax1.set_title('permeability [m^2]')\n",
    "    ax1.set_aspect('equal')\n",
    "    ax1.axis('off')\n",
    "    # cb = fig.colorbar(p1, fraction=0.04, pad=0.04, extend=\"both\", shrink=0.4)\n",
    "    cb.ax.set_title('burn\\nseverity\\n')\n",
    "    # plt.text(np.mean(px), np.mean(py)*1.016, 'fire perimeter', fontsize=14)\n",
    "    # plt.text(np.mean(px)/0.99, np.mean(py)*1.016, 'watershed', fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32ebd74c-0d85-46ef-9ea5-3d3d880a9957",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if fire:\n",
    "#     _iprop = np.copy(iprop)\n",
    "#     for i in range(len(xyz)):\n",
    "#         _iprop[i] = iprop[i]*0.2 if Point(xyz[i,0], xyz[i,1]).within(fireperimeter) else iprop[i]\n",
    "        \n",
    "#     ratio = (iprop-_iprop)/iprop/0.2\n",
    "\n",
    "#     fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(9,6))\n",
    "    \n",
    "#     p1 = ax1.scatter(xyz[:,0], xyz[:,1], c=np.log10(iprop), cmap='PuOr', vmin=-13.25, vmax=-11.25, s=0.005)\n",
    "#     watershed_workflow.plot.hucs(watershed_workflow.split_hucs.SplitHUCs(my_hucs), crs, ax=ax1, color='navy', lw=0.5)\n",
    "#     ax1.plot(px, py, 'firebrick', lw=1)\n",
    "#     ax1.set_title('Pre-fire topsoil\\npermeability [m$^2$]')\n",
    "#     ax1.set_aspect('equal')\n",
    "#     ax1.axis('off')\n",
    "#     cb = fig.colorbar(p1, fraction=0.04, pad=0.04, extend=\"both\", shrink=0.25, ax=ax1)\n",
    "#     cb.ax.set_title('log K\\n')\n",
    "    \n",
    "#     p2 = ax2.scatter(xyz[:,0], xyz[:,1], c=np.log10(_iprop), cmap='PuOr', vmin=-13.25, vmax=-11.25, s=0.005)\n",
    "#     watershed_workflow.plot.hucs(watershed_workflow.split_hucs.SplitHUCs(my_hucs), crs, ax=ax2, color='navy', lw=0.5)\n",
    "#     ax2.plot(px, py, 'firebrick', lw=1)\n",
    "#     ax2.set_title('Post-fire topsoil\\npermeability [m$^2$]')\n",
    "#     ax2.set_aspect('equal')\n",
    "#     ax2.axis('off')\n",
    "#     cb = fig.colorbar(p2, fraction=0.04, pad=0.04, extend=\"both\", shrink=0.25, ax=ax2)\n",
    "#     cb.ax.set_title('log K\\n')\n",
    "    \n",
    "#     p3 = ax3.scatter(xyz[:,0][ratio==1], xyz[:,1][ratio==1], c='green', s=0.005, label='no-low')\n",
    "#     p3 = ax3.scatter(xyz[:,0][ratio==2], xyz[:,1][ratio==2], c='cyan', s=0.005, label='low')\n",
    "#     p3 = ax3.scatter(xyz[:,0][ratio==3], xyz[:,1][ratio==3], c='yellow', s=0.005, label='moderate')\n",
    "#     p3 = ax3.scatter(xyz[:,0][ratio==4], xyz[:,1][ratio==4], c='red', s=0.005, label='high')\n",
    "#     p3 = ax3.scatter(xyz[:,0][ratio==0], xyz[:,1][ratio==0], c='white', s=0.005, label='_nolegend_')\n",
    "#     watershed_workflow.plot.hucs(watershed_workflow.split_hucs.SplitHUCs(my_hucs), crs, ax=ax3, color='navy', lw=0.5)\n",
    "#     ax3.plot(px, py, 'firebrick', lw=1)\n",
    "#     ax3.set_title('(1-Post-fire/Pre-fire)*0.2')\n",
    "#     ax3.set_aspect('equal')\n",
    "#     ax3.axis('off')\n",
    "\n",
    "#     plt.tight_layout()\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60ec9900-1e70-4bde-a40a-912c0b5e14e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "if fire:\n",
    "    from scipy.spatial import distance\n",
    "\n",
    "    def closest_burn_severity(node):\n",
    "        closest_index = distance.cdist([node], burn_severity_nodes).argmin()\n",
    "        return z[closest_index]\n",
    "        # return burn_severity_nodes[closest_index]\n",
    "\n",
    "    closest_burn_severity((xyz[1234,0], xyz[1234,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fee0bf9e-d519-477d-8655-9518205a16bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "if fire:\n",
    "    li = []\n",
    "    for i in range(len(xyz)):\n",
    "        if Point(xyz[i,0], xyz[i,1]).within(fireperimeter):\n",
    "            li.append(i)\n",
    "    print(len(xyz))\n",
    "    print(len(li))\n",
    "    print(soil_color[li])\n",
    "    print(np.unique(soil_color[li]))\n",
    "    print(len(np.unique(soil_color[li])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34d5f2e8-babd-46ea-86f0-c3fc1a5400ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "if fire:\n",
    "    # recompute = True\n",
    "    recompute = True\n",
    "    if recompute:\n",
    "        from tqdm import trange\n",
    "        logging.info('start searching closest burn severity')\n",
    "        _iprop = np.copy(iprop)\n",
    "        for i in trange(len(xyz)):\n",
    "            if Point(xyz[i,0], xyz[i,1]).within(fireperimeter):\n",
    "                 _iprop[i] = iprop[i]*(1-closest_burn_severity((xyz[i,0], xyz[i,1]))*0.2)\n",
    "        logging.info('end searching closest burn severity')\n",
    "        np.savetxt(postfire_perm_file_path, _iprop)\n",
    "    else:\n",
    "        _iprop = np.loadtxt(postfire_perm_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bb331df-d5b8-4457-bd21-bc290bc8b175",
   "metadata": {},
   "outputs": [],
   "source": [
    "if fire:\n",
    "    ratio = np.array(list(int(i) for i in (iprop-_iprop)/iprop/0.2))\n",
    "\n",
    "    fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(9,6))\n",
    "    \n",
    "    p1 = ax1.scatter(xyz[:,0], xyz[:,1], c=np.log10(iprop), cmap='PuOr', vmin=-12.75, vmax=-11, s=0.005)\n",
    "    watershed_workflow.plot.hucs(watershed_workflow.split_hucs.SplitHUCs(my_hucs), crs, ax=ax1, color='navy', lw=0.5)\n",
    "    ax1.plot(px, py, 'firebrick', lw=1)\n",
    "    ax1.set_title('Pre-fire topsoil\\npermeability')\n",
    "    ax1.set_aspect('equal')\n",
    "    ax1.axis('off')\n",
    "    cb = fig.colorbar(p1, fraction=0.04, pad=0.04, extend=\"both\", shrink=0.25, ax=ax1)\n",
    "    cb.ax.set_title('log K\\n[m$^2$]', fontsize=10)\n",
    "    \n",
    "    p2 = ax2.scatter(xyz[:,0], xyz[:,1], c=np.log10(_iprop), cmap='PuOr', vmin=-12.75, vmax=-11, s=0.005)\n",
    "    watershed_workflow.plot.hucs(watershed_workflow.split_hucs.SplitHUCs(my_hucs), crs, ax=ax2, color='navy', lw=0.5)\n",
    "    ax2.plot(px, py, 'firebrick', lw=1)\n",
    "    ax2.set_title('Post-fire topsoil\\npermeability')\n",
    "    ax2.set_aspect('equal')\n",
    "    ax2.axis('off')\n",
    "    cb = fig.colorbar(p2, fraction=0.04, pad=0.04, extend=\"both\", shrink=0.25, ax=ax2)\n",
    "    cb.ax.set_title('log K\\n[m$^2$]', fontsize=10)\n",
    "    \n",
    "    p3 = ax3.scatter(xyz[:,0][ratio==1], xyz[:,1][ratio==1], c='green', s=0.005, label='no-low')\n",
    "    p3 = ax3.scatter(xyz[:,0][ratio==2], xyz[:,1][ratio==2], c='cyan', s=0.005, label='low')\n",
    "    p3 = ax3.scatter(xyz[:,0][ratio==3], xyz[:,1][ratio==3], c='yellow', s=0.005, label='moderate')\n",
    "    p3 = ax3.scatter(xyz[:,0][ratio==4], xyz[:,1][ratio==4], c='red', s=0.005, label='high')\n",
    "    p3 = ax3.scatter(xyz[:,0][ratio==0], xyz[:,1][ratio==0], c='white', s=0.005, label='_nolegend_')\n",
    "    watershed_workflow.plot.hucs(watershed_workflow.split_hucs.SplitHUCs(my_hucs), crs, ax=ax3, color='navy', lw=0.5)\n",
    "    ax3.plot(px, py, 'firebrick', lw=1)\n",
    "    ax3.set_title('difference')\n",
    "    ax3.legend(loc='upper right', edgecolor='none', facecolor='white', framealpha=0.5, markerscale=50, fontsize=9)\n",
    "    ax3.set_aspect('equal')\n",
    "    ax3.axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a76da60c-b537-491e-8dc2-8de4ce7e824c",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(ratio) if fire else fire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58de6f28-f8f1-4808-a25a-75897b984492",
   "metadata": {},
   "outputs": [],
   "source": [
    "if fire:\n",
    "    fig, ax = plt.subplots(1, 1)\n",
    "    plt.hist(np.log10(iprop), bins=50, alpha=0.5, label='prefire mean='+str(np.around(np.nanmean(np.log10(iprop)),4)))\n",
    "    plt.hist(np.log10(_iprop), bins=50, alpha=0.5, label='postfire mean='+str(np.around(np.nanmean(np.log10(_iprop)),4)))\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f53af12-40dc-4081-89bb-3caa187fb240",
   "metadata": {},
   "outputs": [],
   "source": [
    "if fire:\n",
    "    xyzb = np.zeros((len(xyz),4))\n",
    "    xyzb[:,:3] = xyz\n",
    "    xyzb[:,-1] = np.log10(_iprop)\n",
    "    newxyzb = np.copy(xyzb)\n",
    "    newxyzb[:,2] -= 10\n",
    "    stacked = np.vstack((xyzb,newxyzb))\n",
    "    np.savetxt(postfire_perm_4d_file_path, stacked)\n",
    "    print(stacked.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58d92d69",
   "metadata": {
    "papermill": {
     "duration": 1.084966,
     "end_time": "2022-03-22T21:37:23.612110",
     "exception": false,
     "start_time": "2022-03-22T21:37:22.527144",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Note the missing data (white).  This is because some SSURGO map units have no formation with complete \n",
    "# information.  So we merge the above available data, filling where possible and dropping regions that\n",
    "# do not have a complete set of properties.\n",
    "soil_survey_props_clean = soil_survey_props.reset_index()\n",
    "\n",
    "# later scripts expect 'native_index' as a standard name of holding onto the original IDs\n",
    "soil_survey_props_clean.rename_axis('native_index', inplace=True)\n",
    "soil_survey_props_clean.rename(columns={'mukey':'native_index'}, inplace=True)\n",
    "\n",
    "# need thickness in m\n",
    "soil_survey_props_clean['thickness [cm]'] = soil_survey_props_clean['thickness [cm]']/100.\n",
    "soil_survey_props_clean.rename(columns={'thickness [cm]':'thickness [m]'}, inplace=True)\n",
    "\n",
    "\n",
    "def replace_column_nans(df, col_nan, col_replacement):\n",
    "    \"\"\"In a df, replace col_nan entries by col_replacement if is nan.  In Place!\"\"\"\n",
    "    row_indexer = df[col_nan].isna()\n",
    "    df.loc[row_indexer, col_nan] = df.loc[row_indexer, col_replacement]\n",
    "    return\n",
    "\n",
    "# where poro or perm is nan, put Rosetta poro\n",
    "replace_column_nans(soil_survey_props_clean, 'porosity [-]', 'Rosetta porosity [-]')\n",
    "replace_column_nans(soil_survey_props_clean, 'permeability [m^2]', 'Rosetta permeability [m^2]')\n",
    "\n",
    "# drop unnecessary columns\n",
    "for col in ['Rosetta porosity [-]', 'Rosetta permeability [m^2]', 'bulk density [g/cm^3]', 'total sand pct [%]',\n",
    "            'total silt pct [%]', 'total clay pct [%]']:\n",
    "    soil_survey_props_clean.pop(col)\n",
    "    \n",
    "# drop nans\n",
    "soil_survey_props_clean.dropna(inplace=True)\n",
    "soil_survey_props_clean.reset_index(drop=True, inplace=True)\n",
    "\n",
    "assert soil_survey_props_clean['porosity [-]'][:].min() >= min_porosity\n",
    "assert soil_survey_props_clean['permeability [m^2]'][:].max() <= max_permeability\n",
    "soil_survey_props_clean\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ac9aff3",
   "metadata": {
    "papermill": {
     "duration": 29.497919,
     "end_time": "2022-03-22T21:37:54.134522",
     "exception": false,
     "start_time": "2022-03-22T21:37:24.636603",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# create a new soil_color, keeping on those that are kept here and re-indexing to ATS indices\n",
    "soil_color_new = -np.ones_like(soil_color)\n",
    "for new_id, mukey in enumerate(soil_survey_props_clean['native_index']):\n",
    "    soil_color_new[np.where(soil_color == mukey)] = 1000+new_id\n",
    "\n",
    "if generate_plots:\n",
    "    # image the new soil_color\n",
    "    indices, cmap, norm, ticks, labels = watershed_workflow.colors.generate_indexed_colormap(soil_color_new, cmap='tab20c')\n",
    "    fig, ax = watershed_workflow.plot.get_ax(crs)\n",
    "\n",
    "    mp = watershed_workflow.plot.mesh(m2, crs, ax=ax, facecolor='color',\n",
    "                        linewidth=0, color=soil_color_new, \n",
    "                        cmap=cmap, norm=norm)\n",
    "\n",
    "    # watershed_workflow.colors.colorbar_index(ncolors=len(np.unique(soil_color_new)), cmap=cmap, labels=labels) \n",
    "\n",
    "    ax.set_title('soil type index')\n",
    "    ax.axis('off')\n",
    "    fig.savefig(f'../images/{name}/{name}_soil_indices')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04cca31a",
   "metadata": {
    "papermill": {
     "duration": 0.986225,
     "end_time": "2022-03-22T21:37:56.133588",
     "exception": false,
     "start_time": "2022-03-22T21:37:55.147363",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### GLYHMPS geologic layer\n",
    "\n",
    "GLYHMPS is complete in that it does not appear to have missing data, but does not have texture properties needed for Water Retention Models.  Instead we rely on scaling laws to fill the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c05ba23d",
   "metadata": {
    "papermill": {
     "duration": 29.210078,
     "end_time": "2022-03-22T21:38:26.332064",
     "exception": false,
     "start_time": "2022-03-22T21:37:57.121986",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# extract the GLYHMPS geologic structure data as shapes and project it onto the mesh\n",
    "target_bounds = watershed.exterior().bounds\n",
    "logging.info('target bounds: {}'.format(target_bounds))\n",
    "\n",
    "_, geo_survey, geo_survey_props = watershed_workflow.get_shapes(sources['geologic structure'], \n",
    "                                                      target_bounds, crs, crs, properties=True,\n",
    "                                                      min_porosity=min_porosity, \n",
    "                                                      max_permeability=max_permeability,\n",
    "                                                      max_vg_alpha=max_vg_alpha)\n",
    "\n",
    "# -- log the bounds targeted and found\n",
    "# logging.info('shape union bounds: {}'.format(\n",
    "#     shapely.ops.cascaded_union(geo_survey).bounds))\n",
    "\n",
    "# -- determine the ID for each soil unit; this uniquely identifies formation\n",
    "#    properties\n",
    "geo_ids = np.array([shp.properties['id'] for shp in geo_survey], np.int32)\n",
    "\n",
    "# -- color a raster by the polygons (this makes identifying a triangle's value much \n",
    "#    more efficient)\n",
    "geo_color_profile, geo_color_raster = \\\n",
    "            watershed_workflow.color_raster_from_shapes(geo_survey, crs, geo_ids,\n",
    "                                                        target_bounds, 10, crs, -1)\n",
    "\n",
    "# -- resample the raster to the triangles\n",
    "geo_color = watershed_workflow.values_from_raster(m2.centroids, crs, \n",
    "                                         geo_color_raster, geo_color_profile)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a35cb8ed",
   "metadata": {
    "papermill": {
     "duration": 1.101147,
     "end_time": "2022-03-22T21:38:28.478503",
     "exception": false,
     "start_time": "2022-03-22T21:38:27.377356",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# select the properties that appear in the mesh\n",
    "geo_survey_props.set_index('id', inplace=True, drop=False)\n",
    "geo_survey_props = geo_survey_props.loc[np.unique(geo_color), :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaeca781",
   "metadata": {
    "papermill": {
     "duration": 28.766215,
     "end_time": "2022-03-22T21:38:58.284295",
     "exception": false,
     "start_time": "2022-03-22T21:38:29.518080",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if generate_plots:\n",
    "    # plot the geologic formation id\n",
    "    indices, cmap, norm, ticks, labels = watershed_workflow.colors.generate_indexed_colormap(geo_color, cmap='tab20b')\n",
    "\n",
    "    fig, ax = watershed_workflow.plot.get_ax(crs)\n",
    "\n",
    "    mp = watershed_workflow.plot.mesh(m2, crs, ax=ax, facecolor='color',\n",
    "                        linewidth=0, color=geo_color, \n",
    "                        cmap=cmap, norm=norm)\n",
    "\n",
    "    # watershed_workflow.colors.colorbar_index(ncolors=len(np.unique(geo_color)), cmap=cmap, labels=labels) \n",
    "\n",
    "    ax.set_title('geol type index')\n",
    "    ax.axis('off')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c215c704",
   "metadata": {
    "papermill": {
     "duration": 2.581599,
     "end_time": "2022-03-22T21:39:01.959114",
     "exception": false,
     "start_time": "2022-03-22T21:38:59.377515",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if generate_plots:\n",
    "    # plot permeability of the underlying geologic layer\n",
    "    iprop = np.empty(geo_color.shape, 'd')\n",
    "    for i in geo_survey_props.index:\n",
    "        iprop[geo_color == i] = geo_survey_props.loc[i, 'permeability [m^2]']\n",
    "    \n",
    "    fig, ax = watershed_workflow.plot.get_ax(crs)\n",
    "    mp = watershed_workflow.plot.triangulation(mesh_points3, mesh_tris, crs, ax=ax, \n",
    "                                 color=np.log10(iprop), edgecolor='none', cmap='jet')\n",
    "    cbar = fig.colorbar(mp, shrink=0.5)\n",
    "    ax.set_title('geology log permeability [m^2]')\n",
    "    ax.axis('off')\n",
    "\n",
    "    fig.savefig(f'../images/{name}/{name}_geo_permeability')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b3e3b5b",
   "metadata": {
    "papermill": {
     "duration": 2.482113,
     "end_time": "2022-03-22T21:39:05.554890",
     "exception": false,
     "start_time": "2022-03-22T21:39:03.072777",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if generate_plots:\n",
    "    # plot porosity of the geologic layer\n",
    "    iprop = np.empty(geo_color.shape, 'd')\n",
    "    for i in geo_survey_props.index:\n",
    "        iprop[geo_color == i] = geo_survey_props.loc[i, 'porosity [-]']\n",
    "\n",
    "    fig, ax = watershed_workflow.plot.get_ax(crs)\n",
    "    mp = watershed_workflow.plot.triangulation(mesh_points3, mesh_tris, crs, ax=ax, \n",
    "                                 color=iprop, edgecolor='none', cmap='jet')\n",
    "    cbar = fig.colorbar(mp, shrink=0.5)\n",
    "    ax.set_title('geology porosity [-]')\n",
    "    ax.axis('off')\n",
    "    fig.savefig(f'../images/{name}/{name}_geo_porosity')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4f38c8c",
   "metadata": {
    "papermill": {
     "duration": 1.116908,
     "end_time": "2022-03-22T21:39:07.716689",
     "exception": false,
     "start_time": "2022-03-22T21:39:06.599781",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# note there are clearly some common regions -- no need to duplicate those with identical values.\n",
    "geo_survey_props_clean = geo_survey_props.copy()\n",
    "geo_survey_props_clean.pop('logk_stdev [-]')\n",
    "geo_survey_props_clean.rename(columns={'id':'native_index'}, inplace=True)\n",
    "\n",
    "\n",
    "def reindex_remove_duplicates(df, index=None):\n",
    "    \"\"\"Removes duplicates, creating a new index and saving the old index as tuples of duplicate values. In place!\"\"\"\n",
    "    if index is not None:\n",
    "        if index in df:\n",
    "            df.set_index(index, drop=True, inplace=True)\n",
    "    \n",
    "    index_name = df.index.name\n",
    "\n",
    "    # identify duplicate rows\n",
    "    duplicates = list(df.groupby(list(df)).apply(lambda x: tuple(x.index)))\n",
    "\n",
    "    # order is preserved\n",
    "    df.drop_duplicates(inplace=True)\n",
    "    df.reset_index(inplace=True)\n",
    "    df[index_name] = duplicates\n",
    "    return\n",
    "\n",
    "reindex_remove_duplicates(geo_survey_props_clean, 'native_index')\n",
    "assert geo_survey_props_clean['porosity [-]'][:].min() >= min_porosity\n",
    "assert geo_survey_props_clean['permeability [m^2]'][:].max() <= max_permeability\n",
    "assert geo_survey_props_clean['van Genuchten alpha [Pa^-1]'][:].max() <= max_vg_alpha\n",
    "\n",
    "geo_survey_props_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "082fc395",
   "metadata": {
    "papermill": {
     "duration": 27.810322,
     "end_time": "2022-03-22T21:39:36.602327",
     "exception": false,
     "start_time": "2022-03-22T21:39:08.792005",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# create a new geologic layer color, keeping on those that are kept here and re-indexing to ATS indices\n",
    "geo_color_new = -np.ones_like(geo_color)\n",
    "for new_id, old_id_dups in enumerate(geo_survey_props_clean['native_index']):\n",
    "    for old_id in old_id_dups:\n",
    "        geo_color_new[np.where(geo_color == old_id)] = 100+new_id\n",
    "    \n",
    "if generate_plots:\n",
    "    # image the new geo_color\n",
    "    indices, cmap, norm, ticks, labels = watershed_workflow.colors.generate_indexed_colormap(geo_color_new, cmap='tab20c')\n",
    "    fig, ax = watershed_workflow.plot.get_ax(crs)\n",
    "\n",
    "    mp = watershed_workflow.plot.mesh(m2, crs, ax=ax, facecolor='color',\n",
    "                        linewidth=0, color=geo_color_new, \n",
    "                        cmap=cmap, norm=norm)\n",
    "\n",
    "    # watershed_workflow.colors.colorbar_index(ncolors=len(np.unique(geo_color_new)), cmap=cmap, labels=labels) \n",
    "\n",
    "    ax.set_title('geologic type index')\n",
    "    ax.axis('off')\n",
    "    fig.savefig(f'../images/{name}/{name}_geo_indices')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e127af1c-8b31-4640-a0bd-7843d687042d",
   "metadata": {
    "papermill": {
     "duration": 1.087068,
     "end_time": "2022-03-22T21:39:38.705051",
     "exception": false,
     "start_time": "2022-03-22T21:39:37.617983",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "assert(geo_color_new.min() > 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ce5a1ff",
   "metadata": {
    "papermill": {
     "duration": 0.995438,
     "end_time": "2022-03-22T21:39:40.705580",
     "exception": false,
     "start_time": "2022-03-22T21:39:39.710142",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Depth-to-bedrock"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc112023",
   "metadata": {
    "papermill": {
     "duration": 1.052293,
     "end_time": "2022-03-22T21:39:42.756157",
     "exception": false,
     "start_time": "2022-03-22T21:39:41.703864",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Depth to bedrock is taken from the [SoilGrids](http://globalchange.bnu.edu.cn/research/dtb.jsp) product.  Here we download a US-based, clipped version of this global product, as file sizes are quite large (all products potentially used total over 100GB)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e08e74e8",
   "metadata": {
    "papermill": {
     "duration": 1.710116,
     "end_time": "2022-03-22T21:39:45.524153",
     "exception": false,
     "start_time": "2022-03-22T21:39:43.814037",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# DTB_source = watershed_workflow.source_list.structure_sources['SoilGrids2017']\n",
    "DTB_profile, DTB_raster = watershed_workflow.get_raster_on_shape(sources['depth to bedrock'], watershed.exterior(), crs, \n",
    "                                                       nodata=-99999)#, variable='BDTICM')\n",
    "\n",
    "# resample the raster to the triangles\n",
    "DTB_raster = DTB_raster/100 #convert from cm to m\n",
    "DTB = watershed_workflow.values_from_raster(m2.centroids, crs, DTB_raster, DTB_profile, algorithm='piecewise bilinear')\n",
    "DTB = np.where(DTB >= 0, DTB, np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bb9721c",
   "metadata": {
    "papermill": {
     "duration": 2.42185,
     "end_time": "2022-03-22T21:39:48.955368",
     "exception": false,
     "start_time": "2022-03-22T21:39:46.533518",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if generate_plots:\n",
    "    # plot the resulting surface mesh\n",
    "    fig, ax = watershed_workflow.plot.get_ax(crs, window=[0.05,0.1,0.9,0.8])\n",
    "    cbax = fig.add_axes([.95,0.1,0.05,0.8])\n",
    "\n",
    "    mp = watershed_workflow.plot.triangulation(mesh_points3, mesh_tris, crs, ax=ax, \n",
    "                                 color=DTB, cmap='plasma_r', edgecolor='none', linewidth=0.1)\n",
    "    cbar = fig.colorbar(mp, orientation=\"vertical\", cax=cbax)\n",
    "    watershed_workflow.plot.hucs(watershed, crs, ax=ax, color='k', linewidth=1)\n",
    "\n",
    "    ax.set_aspect('equal', 'datalim')\n",
    "    ax.axis('off')\n",
    "\n",
    "    cbar.ax.set_title('DTB [m]')\n",
    "    fig.savefig(f'../images/{name}/{name}_dtb')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b8bd525",
   "metadata": {
    "papermill": {
     "duration": 1.082088,
     "end_time": "2022-03-22T21:39:51.053032",
     "exception": false,
     "start_time": "2022-03-22T21:39:49.970944",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## A combined, complete product?\n",
    "\n",
    "As a default, we would like a material-driven (e.g. not fields for porosity, perm, etc, but soil classes, each with a common porosity/permeability/vG curve) default that is valid everywhere.  That makes it clear that we must rely on GLHYMPS as the only material-based product that is valid everywhere.  Other products may be layered on top of this, replacing GLHYMPS values, but the underlying layer should be based on GLHYMPS.  To fill in the van Genuchten properties, we relate alpha to permeability and choose a single common n and s_r.\n",
    "\n",
    "Where available, we then choose to use SSURGO as a layer on top of GLHYMPS.  So start by using all GLHYMPS values, then override ones where SSURGO is valid with those values.  This will be the second model, and has then three layers -- a bedrock layer, a soil layer from 0 to 2m, and a geologic layer, using GLHYMPS values.  SoilGrids depth-to-bedrock will be used to provide the transition between bedrock and (where > 2m) the GLHYMPS \"geologic\" layer or (where < 2m) the SSURGO \"soil\" layer.  Where SSURGO has no values, the underlying GLHYMPS values will be used even in the top 2m.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38df9e85",
   "metadata": {
    "papermill": {
     "duration": 1.018261,
     "end_time": "2022-03-22T21:39:53.087417",
     "exception": false,
     "start_time": "2022-03-22T21:39:52.069156",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Note, all integer IDs in mesh files must be unique.  This includes Material IDs, side sets, etc.  We create the Material ID map and data frame.  This is used to standardize IDs from multiple data sources.  Traditionally, ATS numbers Material IDs/Side Sets as:\n",
    "\n",
    "* 0-9 : reserved for boundaries, surface/bottom, etc\n",
    "* 10-99 : Land Cover side sets, typically NLCD IDs are used\n",
    "* 100-999 : geologic layer material IDs. 999 is reserved for bedrock.\n",
    "* 1000-9999 : soil layer material IDs\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "631185df",
   "metadata": {
    "papermill": {
     "duration": 1.124434,
     "end_time": "2022-03-22T21:39:55.235194",
     "exception": false,
     "start_time": "2022-03-22T21:39:54.110760",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# map SSURGO mukey to ATS_ID\n",
    "soil_survey_props_clean['ats_id'] = range(1000, 1000+len(soil_survey_props_clean))\n",
    "soil_survey_props_clean.set_index('ats_id', inplace=True)\n",
    "\n",
    "# map GLHYMPS id to ATS_ID\n",
    "geo_survey_props_clean['ats_id'] = range(100, 100+len(geo_survey_props_clean))\n",
    "geo_survey_props_clean.set_index('ats_id', inplace=True)\n",
    "\n",
    "bedrock_props = watershed_workflow.soil_properties.get_bedrock_properties()\n",
    "\n",
    "# merge the properties databases\n",
    "subsurface_props = pandas.concat([geo_survey_props_clean,\n",
    "                                  soil_survey_props_clean,\n",
    "                                  bedrock_props])\n",
    "\n",
    "# save the properties to disk for use in generating input file\n",
    "outputs['subsurface_properties_filename'] = f'../data-processed/{name}/{name}_subsurface_properties.csv'\n",
    "subsurface_props.to_csv(outputs['subsurface_properties_filename'])\n",
    "subsurface_props\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9989b819",
   "metadata": {
    "papermill": {
     "duration": 1.023575,
     "end_time": "2022-03-22T21:39:57.272683",
     "exception": false,
     "start_time": "2022-03-22T21:39:56.249108",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # make a giant plot of all the WRMs\n",
    "# import plot_wrm # $ATS_SRC_DIR/tools/utils\n",
    "# fig = plt.figure()\n",
    "# ax = fig.add_subplot(111)\n",
    "\n",
    "# cm_gl = watershed_workflow.colors.cm_mapper(100, 103, 'winter')\n",
    "# cm_ss = watershed_workflow.colors.cm_mapper(1000,1017, 'autumn')\n",
    "\n",
    "# for i in subsurface_props.index:\n",
    "#     if i < 999:\n",
    "#         cl = cm_gl(i)\n",
    "#     else:\n",
    "#         cl = cm_ss(i)\n",
    "#     alpha = subsurface_props.loc[i]['van Genuchten alpha [Pa^-1]']\n",
    "#     n = subsurface_props.loc[i]['van Genuchten n [-]']\n",
    "#     sr = subsurface_props.loc[i]['residual saturation [-]']\n",
    "#     vg = plot_wrm.VanGenuchten(alpha, n, sr)\n",
    "#     plot_wrm.plot(vg, ax, cl)\n",
    "    \n",
    "# # include wilting point limiters\n",
    "# wp1 = plot_wrm.WiltingPointLimiter(7400, 275000)\n",
    "# wp2 = plot_wrm.WiltingPointLimiter(6600, 255000)\n",
    "# wp3 = plot_wrm.WiltingPointLimiter(3500, 224000)\n",
    "# for wp in [wp1, wp2, wp3]:\n",
    "#     plot_wrm.plot(wp, ax, 'k')\n",
    "\n",
    "    \n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a80ca758-c6c8-40fd-b3c7-a7282ca8c509",
   "metadata": {},
   "source": [
    "## Mesh extrusion\n",
    "\n",
    "Given the surface mesh and material IDs on both the surface and subsurface, we can extrude the surface mesh in the vertical to make a 3D mesh.\n",
    "\n",
    "The most difficult aspect of extrusion is creating meshes that:\n",
    "1. aren't huge numbers of cells\n",
    "2. aren't huge cell thicknesses, especially near the surface\n",
    "3. follow implied interfaces, e.g. bottom of soil and bottom of geologic layer\n",
    "\n",
    "This is an iterative process that requires some care and some art."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c50bfcad",
   "metadata": {
    "papermill": {
     "duration": 1.067695,
     "end_time": "2022-03-22T21:40:01.455648",
     "exception": false,
     "start_time": "2022-03-22T21:40:00.387953",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# here we choose the bottom of the domain to be the maximum of the depth to bedrock.  \n",
    "# This is really up to the user, but we are hard-coding this for this watershed_workflow.\n",
    "dtb_max = np.nanmax(DTB)\n",
    "DTB = np.where(np.isnan(DTB), dtb_max, DTB)\n",
    "\n",
    "total_thickness = np.ceil(DTB.max())\n",
    "print(f'total thickness: {total_thickness} m')\n",
    "\n",
    "# total_thickness = 41.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4c224f5",
   "metadata": {
    "papermill": {
     "duration": 1.10578,
     "end_time": "2022-03-22T21:40:03.600988",
     "exception": false,
     "start_time": "2022-03-22T21:40:02.495208",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Generate a dz structure for the top 2m of soil\n",
    "#\n",
    "# here we try for 10 cells, starting at 5cm at the top and going to 50cm at the bottom of the 2m thick soil\n",
    "dzs, res = watershed_workflow.mesh.optimize_dzs(0.05, 0.5, 2, 8)\n",
    "print(dzs)\n",
    "print(sum(dzs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ad19ef9",
   "metadata": {
    "papermill": {
     "duration": 1.051951,
     "end_time": "2022-03-22T21:40:05.655205",
     "exception": false,
     "start_time": "2022-03-22T21:40:04.603254",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# this looks like it would work out, with rounder numbers:\n",
    "dzs_soil = [0.05, 0.05, 0.05, 0.12, 0.23, 0.5, 0.5, 0.5]\n",
    "print(sum(dzs_soil))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca577847",
   "metadata": {
    "papermill": {
     "duration": 1.132623,
     "end_time": "2022-03-22T21:40:07.866050",
     "exception": false,
     "start_time": "2022-03-22T21:40:06.733427",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 41m total thickness, minus 2m soil thickness, leaves us with 39 meters to make up.\n",
    "# optimize again...\n",
    "dzs2, res2 = watershed_workflow.mesh.optimize_dzs(1, 10, total_thickness-2, 8)\n",
    "print(dzs2)\n",
    "print(sum(dzs2))\n",
    "\n",
    "# how about...\n",
    "# dzs_geo = [1.0, 3.0, 7.0,] + 3*[12.0,]\n",
    "# print(dzs_geo)\n",
    "# print(sum(dzs_geo))\n",
    "\n",
    "dzs_geo = dzs2.astype(int)\n",
    "print(dzs_geo)\n",
    "print(sum(dzs_geo))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d69f2679-d154-442f-afba-22711fd04e1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "if sum(dzs_geo) != total_thickness - 2:\n",
    "    dzs_geo[-1] += total_thickness - 2 - sum(dzs_geo)\n",
    "print(dzs_geo)\n",
    "print(sum(dzs_geo))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8928efc",
   "metadata": {
    "papermill": {
     "duration": 1.068909,
     "end_time": "2022-03-22T21:40:09.979390",
     "exception": false,
     "start_time": "2022-03-22T21:40:08.910481",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# layer extrusion\n",
    "# -- data structures needed for extrusion\n",
    "layer_types = []\n",
    "layer_data = []\n",
    "layer_ncells = []\n",
    "layer_mat_ids = []\n",
    "\n",
    "# -- soil layer --\n",
    "depth = 0\n",
    "for dz in dzs_soil:\n",
    "    depth += 0.5 * dz\n",
    "    layer_types.append('constant')\n",
    "    layer_data.append(dz)\n",
    "    layer_ncells.append(1)\n",
    "    \n",
    "    if use_geologic_layer:\n",
    "        # use glhymps params\n",
    "        br_or_geo = np.where(depth < DTB, geo_color_new, 999)\n",
    "        soil_or_br_or_geo = np.where(np.bitwise_and(soil_color_new > 0, depth < soil_thickness),\n",
    "                                 soil_color_new,\n",
    "                                 br_or_geo)\n",
    "    else:\n",
    "        # use ssurgo down to DTB if it exists\n",
    "        soil_or_geo = np.where(soil_color_new > 0, soil_color_new, geo_color_new)\n",
    "        soil_or_br_or_geo = np.where(depth < DTB, soil_or_geo, 999)\n",
    "    layer_mat_ids.append(soil_or_br_or_geo)\n",
    "    depth += 0.5 * dz\n",
    "    \n",
    "# -- geologic layer --\n",
    "for dz in dzs_geo:\n",
    "    depth += 0.5 * dz\n",
    "    layer_types.append('constant')\n",
    "    layer_data.append(dz)\n",
    "    layer_ncells.append(1)\n",
    "    \n",
    "    if use_geologic_layer:\n",
    "        geo_or_br = np.where(depth < DTB, geo_color_new, 999)\n",
    "    else:\n",
    "        # only soil, no geo\n",
    "        soil_or_geo = np.where(soil_color_new > 0, soil_color_new, geo_color_new)\n",
    "        geo_or_br = np.where(depth < DTB, soil_or_geo, 999)\n",
    "        \n",
    "    layer_mat_ids.append(geo_or_br)\n",
    "    depth += 0.5 * dz\n",
    "\n",
    "# print the summary\n",
    "watershed_workflow.mesh.Mesh3D.summarize_extrusion(layer_types, layer_data, \n",
    "                                            layer_ncells, layer_mat_ids)\n",
    "\n",
    "# downselect subsurface properties to only those that are used\n",
    "layer_mat_id_used = list(np.unique(np.array(layer_mat_ids)))\n",
    "subsurface_props_used = subsurface_props.loc[layer_mat_id_used]\n",
    "len(subsurface_props_used)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe7f0f6b",
   "metadata": {
    "papermill": {
     "duration": 17.925888,
     "end_time": "2022-03-22T21:40:28.940097",
     "exception": false,
     "start_time": "2022-03-22T21:40:11.014209",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# extrude\n",
    "m3 = watershed_workflow.mesh.Mesh3D.extruded_Mesh2D(m2, layer_types, layer_data, \n",
    "                                             layer_ncells, layer_mat_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a01ecbb",
   "metadata": {
    "papermill": {
     "duration": 1.132782,
     "end_time": "2022-03-22T21:40:31.114042",
     "exception": false,
     "start_time": "2022-03-22T21:40:29.981260",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "print('2D labeled sets')\n",
    "print('---------------')\n",
    "for ls in m2.labeled_sets:\n",
    "    print(f'{ls.setid} : {ls.entity} : {len(ls.ent_ids)} : \"{ls.name}\"')\n",
    "\n",
    "print('')\n",
    "print('Extruded 3D labeled sets')\n",
    "print('------------------------')\n",
    "for ls in m3.labeled_sets:\n",
    "    print(f'{ls.setid} : {ls.entity} : {len(ls.ent_ids)} : \"{ls.name}\"')\n",
    "\n",
    "print('')\n",
    "print('Extruded 3D side sets')\n",
    "print('---------------------')\n",
    "for ls in m3.side_sets:\n",
    "    print(f'{ls.setid} : FACE : {len(ls.cell_list)} : \"{ls.name}\"')\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58e5faed",
   "metadata": {
    "papermill": {
     "duration": 64.843155,
     "end_time": "2022-03-22T21:41:36.995682",
     "exception": false,
     "start_time": "2022-03-22T21:40:32.152527",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# save to disk\n",
    "outputs['mesh_filename'] = f'../data-processed/{name}/{name}_{meshsize}-{int(np.round(meshsize*factor))}.exo'\n",
    "try:\n",
    "    os.remove(outputs['mesh_filename'])\n",
    "    os.remove(outputs['mesh_filename'].replace('.exo','.vtk'))\n",
    "except FileNotFoundError:\n",
    "    pass\n",
    "m3.write_exodus(outputs['mesh_filename'])\n",
    "m3.write_vtk(outputs['mesh_filename'].replace('.exo','.vtk'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "059ca31d",
   "metadata": {
    "papermill": {
     "duration": 1.062258,
     "end_time": "2022-03-22T21:41:39.140044",
     "exception": false,
     "start_time": "2022-03-22T21:41:38.077786",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Collect the DayMet raster covering this area\n",
    "\n",
    "Note that here we need two files -- the actual data and the typical year data.\n",
    "\n",
    "The first cell downloads the raw data and generates the actual data file used by ATS, the second cell averages days, smooths the data, and writes a typical year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5cbf210",
   "metadata": {
    "papermill": {
     "duration": 1.134192,
     "end_time": "2022-03-22T21:41:41.317668",
     "exception": false,
     "start_time": "2022-03-22T21:41:40.183476",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# outputs['daymet_filename'] = f'../data-processed/{name}/{name}_DayMet_1980_2023.h5'\n",
    "\n",
    "# if generate_daymet:\n",
    "#     start = \"1-1980\"\n",
    "#     end = \"365-2023\"\n",
    "#     bounds = watershed.exterior().bounds\n",
    "\n",
    "#     dat, x, y = watershed_workflow.daymet.collectDaymet(bounds, crs, start, end)\n",
    "#     ats = watershed_workflow.daymet.daymetToATS(dat)\n",
    "#     attrs = watershed_workflow.daymet.getAttrs(bounds, start, end)\n",
    "#     watershed_workflow.daymet.writeHDF5(ats, x, y, attrs, outputs['daymet_filename'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3f41e0d-e187-444b-8968-9bb52278f0f7",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "outputs['daymet_filename'] = f'../data-processed/{name}/{name}_DayMet_1980_2023.h5'\n",
    "\n",
    "if generate_daymet:\n",
    "    startdate = f\"{start_year}-1-1\"\n",
    "    enddate = f\"{end_year}-12-31\"\n",
    "    bounds = watershed.exterior().bounds\n",
    "    \n",
    "    source = watershed_workflow.sources.manager_daymet.FileManagerDaymet()\n",
    "    data = source.get_data(bounds, crs, startdate, enddate)\n",
    "\n",
    "    assert(len(data.collections) == 1)\n",
    "    met_data = data.collections[0]\n",
    "    met_data_ats = watershed_workflow.daymet.daymet_to_daily_averages(met_data)\n",
    "    attrs = watershed_workflow.daymet.getAttributes(bounds, startdate, enddate)\n",
    "    watershed_workflow.io.write_dataset_to_hdf5(outputs['daymet_filename'], met_data_ats.collections[0], attrs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19bb0d77",
   "metadata": {
    "papermill": {
     "duration": 1.158187,
     "end_time": "2022-03-22T21:41:43.616045",
     "exception": false,
     "start_time": "2022-03-22T21:41:42.457858",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# outputs['daymet_spinup_filename'] = f'../data-processed/{name}/{name}_DayMet_typical_1980_2023.h5'\n",
    "\n",
    "# if generate_daymet:\n",
    "#     ats_typ = watershed_workflow.daymet.daymetToATS(dat, smooth=True, smooth_filter=True, nyears=43)\n",
    "#     watershed_workflow.daymet.writeHDF5(ats_typ, x, y, attrs, outputs['daymet_spinup_filename'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fe090a8-d01f-4a6a-a2fc-d7f8dccaff50",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "outputs['daymet_spinup_filename'] = f'../data-processed/{name}/{name}_DayMet_typical_1980_2023.h5'\n",
    "\n",
    "if generate_daymet:\n",
    "    data_typ = source.get_data(bounds, crs, startdate, enddate)\n",
    "    met_data_typ = data_typ.collections[0]\n",
    "    \n",
    "    logging.info(\"averaging daymet by taking the average for each day across the actual years.\")\n",
    "    window, poly_order = 61, 2\n",
    "    for key in data.collections[0]:\n",
    "        logging.info(f\"smoothing {key} using savgol filter, window = {window} d, poly order = {poly_order}\")\n",
    "        met_data_typ.data[key] = watershed_workflow.utils.compute_average_year(met_data_typ.data[key], output_nyears=end_year-start_year+1, filter=True)    \n",
    "    \n",
    "    met_data_typ_ats = watershed_workflow.daymet.daymet_to_daily_averages(met_data_typ)\n",
    "    watershed_workflow.io.write_dataset_to_hdf5(outputs['daymet_spinup_filename'], met_data_typ_ats.collections[0], attrs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dac38d9",
   "metadata": {
    "papermill": {
     "duration": 1.074099,
     "end_time": "2022-03-22T21:41:45.778290",
     "exception": false,
     "start_time": "2022-03-22T21:41:44.704191",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Generate files for Leaf Area Index\n",
    "\n",
    "In addition to meteorological forcing data, we need land cover leaf area index data.  Typically this can be smoother than the DayMet, and our default source for this, MODIS, is not currently automated.  Therefore this assumes you have already acquired the raw MODIS data (note -- get Pin to add his scripts for this!)\n",
    "\n",
    "MODIS is available from 2002 to present -- we generate a typical year and can use that for all years, though probably we should do something smarter if possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09c929fd-777f-4d59-af93-923b30fb35ec",
   "metadata": {
    "papermill": {
     "duration": 1.118596,
     "end_time": "2022-03-22T21:41:47.955326",
     "exception": false,
     "start_time": "2022-03-22T21:41:46.836730",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot(df, form, axs):\n",
    "    cm = watershed_workflow.colors.enumerated_colors(3)\n",
    "    i = 0\n",
    "    for k in df.keys():\n",
    "        if not k.startswith('time'):\n",
    "            axs[i].plot(df['time [d]'], df[k], form, color=cm[i])\n",
    "            axs[i].set_title(k)\n",
    "            i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f168aae-95c8-4a3a-bd34-c71479f3a111",
   "metadata": {
    "papermill": {
     "duration": 1.182882,
     "end_time": "2022-03-22T21:41:50.216837",
     "exception": false,
     "start_time": "2022-03-22T21:41:49.033955",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# load the raw MODIS data and covert it to pandas\n",
    "ats_raw_modis_filename = f'../data-processed/{name}/{name}_MODIS_LAI_07042002_02102024.h5'\n",
    "\n",
    "if generate_modis:\n",
    "    d = h5py.File(ats_raw_modis_filename,'r')\n",
    "    df = pandas.DataFrame()\n",
    "    for k in d.keys():\n",
    "        df[k] = d[k][:]\n",
    "    df['time [d]'] = df['time [s]']/86400\n",
    "df if generate_modis else generate_modis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "819d19e9-12f9-4486-847c-f2818dfd615d",
   "metadata": {
    "papermill": {
     "duration": 1.119804,
     "end_time": "2022-03-22T21:41:52.423207",
     "exception": false,
     "start_time": "2022-03-22T21:41:51.303403",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if generate_modis:\n",
    "    # interpolate this time series into a daily time series\n",
    "    # ts = np.arange(8214, 14600, 1)\n",
    "    ts = np.arange(df['time [d]'].values[-1]+1)\n",
    "    df_interp = pandas.DataFrame()\n",
    "    df_interp['time [d]'] = ts\n",
    "\n",
    "    for k in df.keys():\n",
    "        if k != 'time [s]':\n",
    "            f = scipy.interpolate.interp1d(df['time [d]'][:], df[k][:])\n",
    "            df_interp[k] = f(ts)\n",
    "\n",
    "    df = df_interp\n",
    "    df['datetime'] = pandas.to_datetime(df['time [d]'], unit='D', origin=pandas.Timestamp('2002-10-01'))\n",
    "df if generate_modis else generate_modis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63ecc29d-c97e-4db0-8891-80d3a393698c",
   "metadata": {},
   "outputs": [],
   "source": [
    "if generate_modis:\n",
    "    leap_index = []\n",
    "    for i in range(len(df)):\n",
    "        if '02-29' in str(df.iloc[i,-1]):\n",
    "            leap_index.append(i)\n",
    "            print(i, df.iloc[i,-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e94df21-0c6d-4237-8db5-c373b47f89db",
   "metadata": {},
   "outputs": [],
   "source": [
    "if generate_modis:\n",
    "    df.drop(leap_index, inplace=True)\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "    df['time [d]'] = np.arange(len(df))\n",
    "df if generate_modis else generate_modis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7f56347-141a-4d76-a5b7-de5eae769bdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "if generate_modis:\n",
    "    for i in range(len(df)):\n",
    "        if '02-29' in str(df.iloc[i,-1]):\n",
    "            print(i, df.iloc[i,-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceaf71b6-f24f-455e-ad3d-6a0a04b9d745",
   "metadata": {},
   "outputs": [],
   "source": [
    "if generate_modis:\n",
    "    df.drop(columns=['datetime'], inplace=True)\n",
    "df if generate_modis else generate_modis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1442c23-d5a3-4f31-ba0d-b384bda6b412",
   "metadata": {
    "papermill": {
     "duration": 1.112997,
     "end_time": "2022-03-22T21:41:54.621148",
     "exception": false,
     "start_time": "2022-03-22T21:41:53.508151",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if generate_modis:\n",
    "    # smooth the data\n",
    "    df_smooth = pandas.DataFrame()\n",
    "    df_smooth['time [d]'] = df['time [d]']\n",
    "    for k in df.keys():\n",
    "        if k != 'time [d]':\n",
    "            df_smooth[k] = scipy.signal.savgol_filter(df[k], 101, 3)\n",
    "\n",
    "    if generate_plots:\n",
    "        df_smooth.iloc[:,1:].plot()\n",
    "        # # plot comparison\n",
    "        # fig = plt.figure()\n",
    "        # axs = fig.subplots(3,1)\n",
    "        # # plot(df, '-', axs)\n",
    "        # plot(df_smooth, '-', axs)\n",
    "        # plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        \n",
    "df_smooth if generate_modis else generate_modis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f12da09f-02f0-4b4d-aedb-df3f6bbb0434",
   "metadata": {
    "papermill": {
     "duration": 1.097268,
     "end_time": "2022-03-22T21:41:56.798470",
     "exception": false,
     "start_time": "2022-03-22T21:41:55.701202",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# add time back and write to disk\n",
    "outputs['modis_filename'] = f'../data-processed/{name}/{name}_MODIS_LAI_07042002_02102024_smoothed.h5'\n",
    "\n",
    "if generate_modis:\n",
    "    df_smooth['time [s]'] = df_smooth['time [d]']*86400\n",
    "    with h5py.File(outputs['modis_filename'],'w') as fid:\n",
    "        for k in df_smooth:\n",
    "            fid.create_dataset(k, data=df_smooth[k][:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "035bed89-354c-4439-8558-8cf1219dda30",
   "metadata": {
    "papermill": {
     "duration": 1.159965,
     "end_time": "2022-03-22T21:41:59.039904",
     "exception": false,
     "start_time": "2022-03-22T21:41:57.879939",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if generate_modis:\n",
    "    df = df_smooth\n",
    "    # split into n_years dataframes, one per year\n",
    "    df_yr = []\n",
    "    for year in range(20):\n",
    "        yr = df.loc[df_interp['time [d]'] >= year*365]\n",
    "        df_yr.append(yr.loc[yr['time [d]'] < (year+1)*365])\n",
    "\n",
    "    # average across the years\n",
    "    df_avg = pandas.DataFrame()\n",
    "    for yr in df_yr:\n",
    "        for k in yr.keys():\n",
    "            if not k.startswith('time'):\n",
    "                if k in df_avg:\n",
    "                    df_avg[k] = df_avg[k].array + yr[k].array\n",
    "                else:\n",
    "                    df_avg[k] = yr[k].copy()\n",
    "\n",
    "    for k in df_avg.keys():\n",
    "        df_avg[k] = df_avg[k][:] / len(df_yr)\n",
    "\n",
    "    df_avg['time [d]'] = df['time [d]']\n",
    "    \n",
    "df_avg if generate_modis else generate_modis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4925f0e-58ea-4d5e-b656-da62dac5173d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if generate_modis:    \n",
    "    if generate_plots:\n",
    "        df_avg.iloc[:,:-1].plot()\n",
    "        # fig = plt.figure()\n",
    "        # axs = fig.subplots(3,1)\n",
    "        # plot(df_avg, '-', axs)\n",
    "        # plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e25d4bf2-86ce-4793-98db-a210d2c5b488",
   "metadata": {
    "papermill": {
     "duration": 1.111957,
     "end_time": "2022-03-22T21:42:01.250709",
     "exception": false,
     "start_time": "2022-03-22T21:42:00.138752",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "nyears = end_year-start_year+1 # to match DayMet 1980-2023\n",
    "if generate_modis:\n",
    "    # replicate nyears times to make nyears years (remem)\n",
    "    # tile all data to repeat n_year times\n",
    "    df_repeat = pandas.DataFrame()\n",
    "    for key in df_avg:\n",
    "        if not key.startswith('time'):\n",
    "            df_repeat[key] = np.tile(df_avg[key].array, nyears)\n",
    "            assert(len(df_repeat) == nyears*365)\n",
    "    \n",
    "    # time is simply daily data\n",
    "    df_repeat['time [d]'] = np.arange(0., nyears * 365., 1.)\n",
    "    df_repeat['time [s]'] = 86400*df_repeat['time [d]']\n",
    "    \n",
    "df_repeat if generate_modis else generate_modis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4adca0a3-1b7c-4e77-84be-d613cf2240c5",
   "metadata": {
    "papermill": {
     "duration": 1.110562,
     "end_time": "2022-03-22T21:42:03.406418",
     "exception": false,
     "start_time": "2022-03-22T21:42:02.295856",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if generate_modis and generate_plots:\n",
    "    df_repeat.iloc[:,:-2].plot()\n",
    "    # plot this and make sure it looks right\n",
    "    # fig = plt.figure()\n",
    "    # axs = fig.subplots(3,1)\n",
    "    # plot(df_repeat, '-', axs)\n",
    "    # plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11313d74-2474-4570-81ab-6114386bacbb",
   "metadata": {
    "papermill": {
     "duration": 1.133908,
     "end_time": "2022-03-22T21:42:05.628466",
     "exception": false,
     "start_time": "2022-03-22T21:42:04.494558",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# write to disk\n",
    "outputs['modis_typical_filename'] = f'../data-processed/{name}/{name}_MODIS_LAI_07042002_02102024_typical{nyears}yr.h5'\n",
    "\n",
    "if generate_modis:\n",
    "    with h5py.File(outputs['modis_typical_filename'],'w') as fid:\n",
    "        for k in df_repeat:\n",
    "            fid.create_dataset(k, data=df_repeat[k][:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91e4da1c-a12e-475b-b0fd-bc4d6badcbcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add 274 to typical\n",
    "if generate_modis:\n",
    "    d = h5py.File(outputs['modis_typical_filename'],'r')\n",
    "    df = pandas.DataFrame()\n",
    "    for k in d.keys():\n",
    "        if k != 'time [d]':\n",
    "            df[k] = d[k][:]\n",
    "    d.close()\n",
    "    df['time [s]'] += 274*86400\n",
    "    with h5py.File(outputs['modis_typical_filename'],'w') as fid:\n",
    "        for k in df:\n",
    "            fid.create_dataset(k, data=df[k][:])\n",
    "df if generate_modis else generate_modis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31f9607d-a696-4b27-94b9-de44bc0fb04e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add 274 to smoothed\n",
    "if generate_modis:\n",
    "    d = h5py.File(outputs['modis_filename'],'r')\n",
    "    df = pandas.DataFrame()\n",
    "    for k in d.keys():\n",
    "        if k != 'time [d]':\n",
    "            df[k] = d[k][:]\n",
    "    d.close()\n",
    "    df['time [s]'] += (274+365*22)*86400\n",
    "    with h5py.File(outputs['modis_filename'],'w') as fid:\n",
    "        for k in df:\n",
    "            fid.create_dataset(k, data=df[k][:])\n",
    "df if generate_modis else generate_modis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcbabe85",
   "metadata": {
    "papermill": {
     "duration": 1.075635,
     "end_time": "2022-03-22T21:42:07.833137",
     "exception": false,
     "start_time": "2022-03-22T21:42:06.757502",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Write ATS input files\n",
    "\n",
    "We now generate three input files -- two for spinup (steadystate solution and cyclic steadystate solution) and one for transient runs.\n",
    "\n",
    "Steadystate has its own physics, but cyclic steadystate and transient share a common set of physics.  Each have their own met data strategy.\n",
    "\n",
    "The first step is to generate the sections of xml that will replace parts of the template files.  This is done prior to loading any templates to make clear that these are totally generated from scratch using the ats_input_spec tool.\n",
    "\n",
    "Note that throughout, we will assume an additional level of folder nesting, e.g. runs will be completed in '../spinup-CoalCreek/run0', meaning that we have to append an extra '../' to the start of all filenames.  This makes it easier to deal with mistakes, continued runs, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad766d0c",
   "metadata": {
    "papermill": {
     "duration": 1.122699,
     "end_time": "2022-03-22T21:42:10.047398",
     "exception": false,
     "start_time": "2022-03-22T21:42:08.924699",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# add the subsurface and surface domains\n",
    "#\n",
    "# Note this also adds a \"computational domain\" region to the region list, and a vis spec \n",
    "# for \"domain\"\n",
    "def add_domains(main_list, mesh_filename, surface_region='surface', snow=True, canopy=True):\n",
    "    ats_input_spec.public.add_domain(main_list, \n",
    "                                 domain_name='domain', \n",
    "                                 dimension=3, \n",
    "                                 mesh_type='read mesh file',\n",
    "                                 mesh_args={'file':mesh_filename})\n",
    "    if surface_region:\n",
    "        main_list['mesh']['domain']['build columns from set'] = surface_region    \n",
    "    \n",
    "        # Note this also adds a \"surface domain\" region to the region list and a vis spec for \n",
    "        # \"surface\"\n",
    "        ats_input_spec.public.add_domain(main_list,\n",
    "                                domain_name='surface',\n",
    "                                dimension=2,\n",
    "                                mesh_type='surface',\n",
    "                                mesh_args={'surface sideset name':'surface'})\n",
    "    if snow:\n",
    "        # Add the snow and canopy domains, which are aliases to the surface\n",
    "        ats_input_spec.public.add_domain(main_list,\n",
    "                                domain_name='snow',\n",
    "                                dimension=2,\n",
    "                                mesh_type='aliased',\n",
    "                                mesh_args={'target':'surface'})\n",
    "    if canopy:\n",
    "        ats_input_spec.public.add_domain(main_list,\n",
    "                                domain_name='canopy',\n",
    "                                dimension=2,\n",
    "                                mesh_type='aliased',\n",
    "                                mesh_args={'target':'surface'})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dace57f3",
   "metadata": {
    "papermill": {
     "duration": 1.105062,
     "end_time": "2022-03-22T21:42:12.213562",
     "exception": false,
     "start_time": "2022-03-22T21:42:11.108500",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def add_land_cover(main_list):\n",
    "    # next write a land-cover section for each NLCD type\n",
    "    for index, nlcd_name in zip(nlcd_indices, nlcd_labels):\n",
    "        ats_input_spec.public.set_land_cover_default_constants(main_list, nlcd_name)\n",
    "\n",
    "    land_cover_list = main_list['state']['initial conditions']['land cover types']\n",
    "    # update some defaults\n",
    "    # ['Other', 'Deciduous Forest', 'Evergreen Forest', 'Shrub/Scrub']\n",
    "    # note, these are from the CLM Technical Note v4.5\n",
    "    #\n",
    "    # Rooting depth curves from CLM TN 4.5 table 8.3\n",
    "    #\n",
    "    # Note, the mafic potential values are likely pretty bad for the types of van Genuchten \n",
    "    # curves we are using (ETC -- add paper citation about this topic).  Likely they need\n",
    "    # to be modified.  Note that these values are in [mm] from CLM TN 4.5 table 8.1, so the \n",
    "    # factor of 10 converts to [Pa]\n",
    "    #\n",
    "    # Note, albedo of canopy taken from CLM TN 4.5 table 3.1\n",
    "    if 42 in nlcd_color_new:\n",
    "        land_cover_list['Evergreen Forest']['rooting profile alpha [-]'] = 7.0\n",
    "        land_cover_list['Evergreen Forest']['rooting profile beta [-]'] = 2.0\n",
    "        land_cover_list['Evergreen Forest']['rooting depth max [m]'] = 10.0\n",
    "        land_cover_list['Evergreen Forest']['mafic potential at fully closed stomata [Pa]'] = 255000\n",
    "        land_cover_list['Evergreen Forest']['mafic potential at fully open stomata [Pa]'] = 66000 * .1\n",
    "        land_cover_list['Evergreen Forest']['albedo of canopy [-]'] = 0.07\n",
    "\n",
    "    if 41 in nlcd_color_new:\n",
    "        land_cover_list['Deciduous Forest']['rooting profile alpha [-]'] = 6.0\n",
    "        land_cover_list['Deciduous Forest']['rooting profile beta [-]'] = 2.0\n",
    "        land_cover_list['Deciduous Forest']['rooting depth max [m]'] = 10.0\n",
    "        land_cover_list['Deciduous Forest']['mafic potential at fully closed stomata [Pa]'] = 224000\n",
    "        land_cover_list['Deciduous Forest']['mafic potential at fully open stomata [Pa]'] = 35000 * .10\n",
    "        land_cover_list['Deciduous Forest']['albedo of canopy [-]'] = 0.1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ede914b",
   "metadata": {
    "papermill": {
     "duration": 1.452061,
     "end_time": "2022-03-22T21:42:14.758742",
     "exception": false,
     "start_time": "2022-03-22T21:42:13.306681",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# add soil sets: note we need a way to name the set, so we use, e.g. SSURGO-MUKEY.\n",
    "def soil_set_name(ats_id):\n",
    "    if ats_id == 999:\n",
    "        return 'bedrock'\n",
    "    source = subsurface_props_used.loc[ats_id]['source']\n",
    "    native_id = subsurface_props_used.loc[ats_id]['native_index']\n",
    "    if type(native_id) in [tuple,list]:\n",
    "        native_id = native_id[0]\n",
    "    return f\"{source}-{native_id}\"\n",
    "\n",
    "\n",
    "# get an ATS \"main\" input spec list -- note, this is a dummy and is not used to write any files yet\n",
    "def get_main():\n",
    "    main_list = ats_input_spec.public.get_main()\n",
    "    flow_pk = ats_input_spec.public.add_leaf_pk(main_list, 'flow', main_list['cycle driver']['PK tree'], \n",
    "                                            'richards-spec')\n",
    "\n",
    "    # add the mesh and all domains\n",
    "    mesh_filename = os.path.join('.', outputs['mesh_filename'])\n",
    "    add_domains(main_list, mesh_filename)\n",
    "\n",
    "    # add labeled sets\n",
    "    for ls in m3.labeled_sets:\n",
    "        ats_input_spec.public.add_region_labeled_set(main_list, ls.name, ls.setid, mesh_filename, ls.entity)\n",
    "    for ss in m3.side_sets:\n",
    "        ats_input_spec.public.add_region_labeled_set(main_list, ss.name, ss.setid, mesh_filename, 'FACE')\n",
    "    \n",
    "    # add land cover\n",
    "    add_land_cover(main_list)\n",
    "\n",
    "    # add soil material ID regions, porosity, permeability, and WRMs\n",
    "    for ats_id in subsurface_props_used.index:\n",
    "        props = subsurface_props_used.loc[ats_id]\n",
    "        set_name = soil_set_name(ats_id)\n",
    "        \n",
    "        if props['van Genuchten n [-]'] < 1.5:\n",
    "            smoothing_interval = 0.01\n",
    "        else:\n",
    "            smoothing_interval = 0.0\n",
    "        \n",
    "        ats_input_spec.public.add_soil_type(main_list, set_name, ats_id, mesh_filename,\n",
    "                                            float(props['porosity [-]']),\n",
    "                                            float(props['permeability [m^2]']), 1.e-7,\n",
    "                                            float(props['van Genuchten alpha [Pa^-1]']),\n",
    "                                            float(props['van Genuchten n [-]']),\n",
    "                                            float(props['residual saturation [-]']),\n",
    "                                            float(smoothing_interval))\n",
    "    # print(main_list)  \n",
    "    # add observations for each subcatchment for transient runs\n",
    "    ats_input_spec.public.add_observations_water_balance(main_list, \"computational domain\", \n",
    "                                                         \"surface domain\",\"external_sides\")\n",
    "    for poly in watershed.polygons():\n",
    "        region = poly.properties[huc_key]\n",
    "        ats_input_spec.public.add_observations_water_balance(main_list, region, outlet_region=region+' outlet')\n",
    "    \n",
    "    \n",
    "    return main_list\n",
    "\n",
    "# create the main list\n",
    "main_list = get_main()\n",
    "\n",
    "outputs['generated_ats'] = f'../data-processed/{name}/{name}_generated_ats.xml'\n",
    "ats_input_spec.io.write(main_list, outputs['generated_ats'])\n",
    "main_xml = ats_input_spec.io.to_xml(main_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c2d6e87",
   "metadata": {
    "papermill": {
     "duration": 1.113642,
     "end_time": "2022-03-22T21:42:16.946196",
     "exception": false,
     "start_time": "2022-03-22T21:42:15.832554",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def populate_basic_properties(xml, main_xml, homogeneous_wrm=False, homogeneous_poro=False, homogeneous_perm=False):\n",
    "    \"\"\"This function updates an xml object with the above properties for mesh, regions, soil props, and lc props\"\"\"\n",
    "    # find and replace the mesh list\n",
    "    mesh_i = next(i for (i,el) in enumerate(xml) if el.get('name') == 'mesh')\n",
    "    xml[mesh_i] = asearch.child_by_name(main_xml, 'mesh')\n",
    "\n",
    "    # find and replace the regions list\n",
    "    region_i = next(i for (i,el) in enumerate(xml) if el.get('name') == 'regions')\n",
    "    xml[region_i] = asearch.child_by_name(main_xml, 'regions')\n",
    "\n",
    "    # find and replace the WRMs list -- note here we only replace the inner \"WRM parameters\" because the\n",
    "    # demo has this in the PK, not in the evaluators list\n",
    "    if not homogeneous_wrm:\n",
    "        wrm_list = asearch.find_path(xml, ['PKs', 'water retention evaluator'])\n",
    "        wrm_i = next(i for (i,el) in enumerate(wrm_list) if el.get('name') == 'WRM parameters')\n",
    "        wrm_list[wrm_i] = asearch.find_path(main_xml, ['PKs','water retention evaluator','WRM parameters'])\n",
    "\n",
    "    fe_list = asearch.find_path(xml, ['state', 'evaluators'])\n",
    "\n",
    "    # find and replace porosity, permeability\n",
    "    if not homogeneous_poro:\n",
    "        poro_i = next(i for (i,el) in enumerate(fe_list) if el.get('name') == 'base_porosity')\n",
    "        fe_list[poro_i] = asearch.find_path(main_xml, ['state', 'evaluators', 'base_porosity'])\n",
    "\n",
    "    if not homogeneous_perm:\n",
    "        perm_i = next(i for (i,el) in enumerate(fe_list) if el.get('name') == 'permeability')\n",
    "        fe_list[perm_i] = asearch.find_path(main_xml, ['state', 'evaluators', 'permeability'])\n",
    "\n",
    "    # find and replace land cover\n",
    "    consts_list = asearch.find_path(xml, ['state', 'initial conditions'])\n",
    "    try:\n",
    "        lc_i = next(i for (i,el) in enumerate(consts_list) if el.get('name') == 'land cover types')\n",
    "    except StopIteration:\n",
    "        pass\n",
    "    else:\n",
    "        consts_list[lc_i] = asearch.find_path(main_xml, ['state', 'initial conditions', 'land cover types'])\n",
    "\n",
    "def create_unique_name(name, homogeneous_wrm=False, homogeneous_poro=False, homogeneous_perm=False):\n",
    "    suffix = '_h'\n",
    "    if homogeneous_perm:\n",
    "        suffix += 'K'\n",
    "    if homogeneous_poro:\n",
    "        suffix += 'p'\n",
    "    if homogeneous_wrm:\n",
    "        suffix += 'w'\n",
    "    if suffix == '_h':\n",
    "        suffix = ''\n",
    "    return name + suffix\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a374e7a7",
   "metadata": {
    "papermill": {
     "duration": 1.094394,
     "end_time": "2022-03-22T21:42:19.133864",
     "exception": false,
     "start_time": "2022-03-22T21:42:18.039470",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "For the first file, we load a spinup template and write the needed quantities into that file, saving it to the appropriate run directory.  Note there is no DayMet or land cover or LAI properties needed for this run.  The only property that is needed is the domain-averaged, mean annual rainfall rate.  We then take off some for ET (note too wet spins up faster than too dry, so don't take off too much...)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "354ba24b",
   "metadata": {
    "papermill": {
     "duration": 34.71578,
     "end_time": "2022-03-22T21:42:54.900416",
     "exception": false,
     "start_time": "2022-03-22T21:42:20.184636",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if generate_daymet:\n",
    "    # calculate the basin-averaged, annual-averaged precip rate\n",
    "    precip_total = ats_typ['precipitation rain [m s^-1]'] + ats_typ['precipitation snow [m SWE s^-1]']\n",
    "    mean_precip = precip_total.mean()\n",
    "else:\n",
    "    rain = 'precipitation rain [m s^-1]'\n",
    "    snow = 'precipitation snow [m SWE s^-1]'\n",
    "    try:\n",
    "        with h5py.File(outputs['daymet_spinup_filename'], 'r') as fid:\n",
    "            mean_precip = np.array([fid[rain][ts][:].mean() + fid[snow][ts][:].mean() for ts in fid[rain].keys()]).mean()\n",
    "    except:\n",
    "        mean_precip = 5e-8\n",
    "        \n",
    "logging.info(f'Mean annual precip rate [m s^-1] = {mean_precip}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8c6d63a",
   "metadata": {
    "papermill": {
     "duration": 1.192077,
     "end_time": "2022-03-22T21:42:57.174136",
     "exception": false,
     "start_time": "2022-03-22T21:42:55.982059",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def write_spinup_steadystate(name, mean_precip, **kwargs):\n",
    "    name = create_unique_name(name, **kwargs)\n",
    "    logging.info(f'Writing transient: {name}')\n",
    "    \n",
    "    # write the spinup xml file\n",
    "    # load the template file\n",
    "    xml = aio.fromFile('spinup_steadystate-template.xml')\n",
    "\n",
    "    # populate basic properties for mesh, regions, and soil properties\n",
    "    populate_basic_properties(xml, main_xml, **kwargs)\n",
    "\n",
    "    # set the mean avg source as 60% of mean precip\n",
    "    precip_el = asearch.find_path(xml, ['state', 'evaluators', 'surface-precipitation', \n",
    "                                        'function-constant', 'value'])\n",
    "    precip_el.setValue(mean_precip * .6)\n",
    "\n",
    "    # write to disk\n",
    "    outputs[f'spinup_steadystate_{name}_filename'] = f'../run0-spinup_steadystate/{name}_{meshsize}-{int(np.round(meshsize*factor))}.xml'\n",
    "    aio.toFile(xml, outputs[f'spinup_steadystate_{name}_filename'])\n",
    "\n",
    "    # make a run directory\n",
    "    outputs[f'spinup_steadystate_{name}_rundir'] = f'../run0-spinup_steadystate/{name}-0'\n",
    "    try:\n",
    "        os.mkdir(outputs[f'spinup_steadystate_{name}_rundir'])\n",
    "    except FileExistsError:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "826f5a99",
   "metadata": {
    "papermill": {
     "duration": 1.066583,
     "end_time": "2022-03-22T21:42:59.308368",
     "exception": false,
     "start_time": "2022-03-22T21:42:58.241785",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "For the second file, we load a transient run template.  This file needs the basics, plus DayMet and LAI as the \"typical year data\".  Also we set the run directory that will be used for the steadystate run."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe3d6e3c",
   "metadata": {
    "papermill": {
     "duration": 1.102115,
     "end_time": "2022-03-22T21:43:01.466696",
     "exception": false,
     "start_time": "2022-03-22T21:43:00.364581",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "For the third file, we load a transient run template as well.  This file needs the basics, DayMet with the actual data, and we choose for this run to use the MODIS typical year.  MODIS is only available for 2002 on, so if we didn't need 1980-2002 we could use the real data, but for this run we want a longer record."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af35d276",
   "metadata": {
    "papermill": {
     "duration": 1.143978,
     "end_time": "2022-03-22T21:43:03.745814",
     "exception": false,
     "start_time": "2022-03-22T21:43:02.601836",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def write_transient(name, cyclic_steadystate=False, start_year=1980, end_year=2023, **kwargs):\n",
    "    # make a unique name based on options\n",
    "    name = create_unique_name(name, **kwargs)\n",
    "    logging.info(f'Writing transient: {name}')\n",
    "\n",
    "    if cyclic_steadystate:\n",
    "        prefix = 'spinup_cyclic'\n",
    "        # start_year = 1980\n",
    "        # end_year = 2023\n",
    "        previous = 'spinup_steadystate'\n",
    "        runnum = 'run1'\n",
    "    else:\n",
    "        prefix = 'transient'\n",
    "        previous = 'spinup_cyclic'\n",
    "        runnum = 'run2'\n",
    "\n",
    "    template_filename = f'{prefix}-template.xml'\n",
    "    \n",
    "    # write the cyclic spinup xml file\n",
    "    # load the template file\n",
    "    xml = aio.fromFile(template_filename)\n",
    "\n",
    "    # populate basic properties for mesh, regions, and soil properties\n",
    "    populate_basic_properties(xml, main_xml, **kwargs)\n",
    "\n",
    "    # update the DayMet filenames\n",
    "    if cyclic_steadystate:\n",
    "        daymet_filename = outputs['daymet_spinup_filename']\n",
    "    else:\n",
    "        daymet_filename = outputs['daymet_filename']\n",
    "    for var in ['surface-incoming_shortwave_radiation',\n",
    "                'surface-precipitation_rain',\n",
    "                'snow-precipitation',\n",
    "                'surface-air_temperature',\n",
    "                'surface-vapor_pressure_air',\n",
    "                'surface-temperature',\n",
    "                'canopy-temperature']:\n",
    "        try:\n",
    "            par = asearch.find_path(xml, ['state', 'evaluators', var, 'file'])\n",
    "        except aerrors.MissingXMLError:\n",
    "            pass\n",
    "        else:\n",
    "            par.setValue(os.path.join('.', daymet_filename))\n",
    "\n",
    "    # update the LAI filenames\n",
    "    for par in asearch.findall_path(xml, ['canopy-leaf_area_index', 'file']):\n",
    "        if cyclic_steadystate:\n",
    "            par.setValue(os.path.join('.', outputs['modis_typical_filename']))\n",
    "        else:\n",
    "            par.setValue(os.path.join('.', outputs['modis_filename']))\n",
    "    \n",
    "    # update the start and end time -- start at Oct 1 of year 0, end 10 years later\n",
    "    start_day = 274 + 365*(start_year - 1980)\n",
    "    par = asearch.find_path(xml, ['cycle driver', 'start time'])\n",
    "    par.setValue(start_day)\n",
    "\n",
    "    end_day = 274 + 365*(end_year - 1980)\n",
    "    par = asearch.find_path(xml, ['cycle driver', 'end time'])\n",
    "    par.setValue(end_day)\n",
    "    \n",
    "    # update the restart filenames\n",
    "    for var in asearch.findall_path(xml, ['initial condition', 'restart file']):\n",
    "        var.setValue(os.path.join('.', outputs[f'{previous}_{name}_rundir'], 'checkpoint_final.h5'))\n",
    "\n",
    "    # update the observations list\n",
    "    obs = next(i for (i,el) in enumerate(xml) if el.get('name') == 'observations')\n",
    "    xml[obs] = asearch.child_by_name(main_xml, 'observations')\n",
    "   \n",
    "        \n",
    "    # write to disk and make a directory for running the run\n",
    "    outputs[f'{prefix}_{name}_filename'] = f'../{runnum}-{prefix}/{name}_{meshsize}-{int(np.round(meshsize*factor))}.xml'\n",
    "    filename = outputs[f'{prefix}_{name}_filename']\n",
    "\n",
    "    outputs[f'{prefix}_{name}_rundir'] = f'../{runnum}-{prefix}/{name}-0'\n",
    "    rundir = outputs[f'{prefix}_{name}_rundir']\n",
    "\n",
    "    aio.toFile(xml, filename)\n",
    "    try:\n",
    "        os.mkdir(rundir)\n",
    "    except FileExistsError:\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb8087ee",
   "metadata": {
    "papermill": {
     "duration": 1.432703,
     "end_time": "2022-03-22T21:43:06.276337",
     "exception": false,
     "start_time": "2022-03-22T21:43:04.843634",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# create the fully-heterogeneous runs\n",
    "if include_heterogeneous:\n",
    "    write_spinup_steadystate(name, mean_precip)\n",
    "    write_transient(name, True)\n",
    "    write_transient(name, False)\n",
    "\n",
    "# create homogeneous runs\n",
    "if include_homogeneous:\n",
    "    write_spinup_steadystate(name, mean_precip, homogeneous_wrm=True, homogeneous_poro=True, homogeneous_perm=True)\n",
    "    write_transient(name, True, homogeneous_wrm=True, homogeneous_poro=True, homogeneous_perm=True)\n",
    "    write_transient(name, False, homogeneous_wrm=True, homogeneous_poro=True, homogeneous_perm=True)\n",
    "    \n",
    "if include_homogeneous_wrm:\n",
    "    write_spinup_steadystate(name, mean_precip, homogeneous_wrm=True, homogeneous_poro=False, homogeneous_perm=False)\n",
    "    write_transient(name, True, homogeneous_wrm=True, homogeneous_poro=False, homogeneous_perm=False)\n",
    "    write_transient(name, False, homogeneous_wrm=True, homogeneous_poro=False, homogeneous_perm=False)\n",
    "    \n",
    "if include_homogeneous_wrm_porosity:\n",
    "    write_spinup_steadystate(name, mean_precip, homogeneous_wrm=True, homogeneous_poro=True, homogeneous_perm=False)\n",
    "    write_transient(name, True, homogeneous_wrm=True, homogeneous_poro=True, homogeneous_perm=False)\n",
    "    write_transient(name, False, homogeneous_wrm=True, homogeneous_poro=True, homogeneous_perm=False)\n",
    "    \n",
    "if include_homogeneous_wrm_permeability:\n",
    "    write_spinup_steadystate(name, mean_precip, homogeneous_wrm=True, homogeneous_poro=False, homogeneous_perm=True)\n",
    "    write_transient(name, True, homogeneous_wrm=True, homogeneous_poro=False, homogeneous_perm=True)\n",
    "    write_transient(name, False, homogeneous_wrm=True, homogeneous_poro=False, homogeneous_perm=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d19d3cb8",
   "metadata": {
    "papermill": {
     "duration": 1.207184,
     "end_time": "2022-03-22T21:43:08.554340",
     "exception": false,
     "start_time": "2022-03-22T21:43:07.347156",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "logging.info('this workflow is a total success')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db14e954-b6d7-4f78-8eeb-8024901029e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f23e2f2-220f-4aa2-bbc0-82903c20f58f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 654.42164,
   "end_time": "2022-03-22T21:43:12.294817",
   "environment_variables": {},
   "exception": null,
   "input_path": "full_workflow_master.ipynb",
   "output_path": "full_workflow_EastTaylor.ipynb",
   "parameters": {
    "hucs": "[14020001,]",
    "name": "EastTaylor",
    "prune_by_area_fraction": 0.002
   },
   "start_time": "2022-03-22T21:32:17.873177",
   "version": "2.3.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
